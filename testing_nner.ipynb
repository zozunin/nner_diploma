{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1RIN2YTAY2f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizerFast, BertConfig, BertModel\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch import nn, optim, cuda, Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from natasha import NewsSyntaxParser, NewsEmbedding, Doc, Segmenter\n",
        "\n",
        "from razdel import sentenize, tokenize\n",
        "import re\n",
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua3Dufi6PE_S"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuwroOUCgHc2",
        "outputId": "72afb111-ae46-4407-f055-b0601a41504d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3eWlCS3QezH"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger('Testing Number Version')\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aqCNAk9QezI"
      },
      "outputs": [],
      "source": [
        "format_logger = logging.Formatter(\n",
        "    '%(message)s')\n",
        "\n",
        "filehandler_logger= logging.FileHandler('/home/logs/testing_number_copy_35.log')\n",
        "\n",
        "filehandler_logger.setFormatter(format_logger)\n",
        "logger.addHandler(filehandler_logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORZWMii99_j8"
      },
      "source": [
        "# Dataset utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm6nVost9_CN"
      },
      "outputs": [],
      "source": [
        "# only evaluation and training\n",
        "class dataset(Dataset):\n",
        "    '''\n",
        "    Возвращает закодированную последовательность:\n",
        "    индексы\n",
        "    маску текста\n",
        "    матрицу синтаксической смежности (для AGGCN)\n",
        "    '''\n",
        "    def __init__(self, texts, span_labels=None, max_len=None):\n",
        "        self.len = len(texts)\n",
        "        self.data = texts\n",
        "        self.span_labels = span_labels\n",
        "        self.max_len = max_len\n",
        "        self.bert_tokenizer = BertTokenizerFast.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "        emb = NewsEmbedding()\n",
        "        self.syntax_parser = NewsSyntaxParser(emb)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        temp_text = tokenize(self.data['Contents'][index])\n",
        "                \n",
        "        tokenized_text = [tok.text for tok in temp_text][:self.max_len]\n",
        "\n",
        "        input_ids = [0] * self.max_len\n",
        "        attention_mask = [0] * self.max_len\n",
        "        for idx, word in enumerate(tokenized_text):\n",
        "            input_ids[idx] = self.bert_tokenizer.encode(word,add_special_tokens=False)[0]\n",
        "            attention_mask[idx] = 1\n",
        "\n",
        "        markup = self.syntax_parser(tokenized_text).tokens\n",
        "        adj_matrix = np.zeros((self.max_len, self.max_len))\n",
        "        for tok in markup:\n",
        "            i = int(tok.id)-1\n",
        "            j = int(tok.head_id)-1\n",
        "            if i < 0 or j < 0:\n",
        "                continue\n",
        "            adj_matrix[i][j] = 1\n",
        "            adj_matrix[j][i] = 1\n",
        "        \n",
        "        # сохраняем путь до нужных данных, чтобы при валидации найти нужную часть датафрейма\n",
        "        # т.о. нет необходимости сохранять все в тензоре одного размера\n",
        "        address_true_spans = [self.data['TextID'][index],\n",
        "                              self.data['SentID'][index]]\n",
        "        \n",
        "        item = {'input_ids': torch.as_tensor(input_ids),\n",
        "                'mask': torch.as_tensor(attention_mask),\n",
        "                'adj': torch.as_tensor(adj_matrix),\n",
        "                'address': address_true_spans}        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lrmPt3Aog2m"
      },
      "outputs": [],
      "source": [
        "def get_ind_sequence(dataframe):\n",
        "\n",
        "    '''\n",
        "    Возвращает последовательность токенов в виде индексов начала и конца этого токена\n",
        "    '''\n",
        "\n",
        "    temp_df = {'TextID':[], 'SentID':[], 'Indices':[]}\n",
        "    for row in range(len(dataframe)):\n",
        "        textid = dataframe['TextID'].iloc[row]\n",
        "        sentid = dataframe['SentID'].iloc[row]\n",
        "        content = dataframe['Contents'].iloc[row]\n",
        "\n",
        "        saved_indices = [[tok.start, tok.stop] for tok in tokenize(content)]\n",
        "        temp_df['TextID'].append(textid)\n",
        "        temp_df['SentID'].append(sentid)\n",
        "        temp_df['Indices'].append(saved_indices)\n",
        "    \n",
        "    df = pd.DataFrame(temp_df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWiAByyWsf4o"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PKZ8SghsmZ_"
      },
      "outputs": [],
      "source": [
        "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n",
        "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n",
        "\n",
        "    positive_matrix = label_matrix.triu(diagonal=1)\n",
        "    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n",
        "\n",
        "    similarity_matrix = similarity_matrix.view(-1)\n",
        "    positive_matrix = positive_matrix.view(-1)\n",
        "    negative_matrix = negative_matrix.view(-1)\n",
        "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qto8PQLPsohP"
      },
      "outputs": [],
      "source": [
        "class CircleLoss(nn.Module):\n",
        "    def __init__(self, m: float, gamma: float) -> None:\n",
        "        super(CircleLoss, self).__init__()\n",
        "        self.m = m\n",
        "        self.gamma = gamma\n",
        "        self.soft_plus = nn.Softplus()\n",
        "\n",
        "    def forward(self, sp: Tensor, sn: Tensor) -> Tensor:\n",
        "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n",
        "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
        "\n",
        "        delta_p = 1 - self.m\n",
        "        delta_n = self.m\n",
        "\n",
        "        logit_p = - ap * (sp - delta_p) * self.gamma\n",
        "        logit_n = an * (sn - delta_n) * self.gamma\n",
        "\n",
        "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERn8KXms1rtk"
      },
      "source": [
        "# DataEvalUtils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lacggzO51bLt"
      },
      "outputs": [],
      "source": [
        "def get_batch_labels(batch_addresses: List, spans_dataset: pd.DataFrame, indices_dataset: pd.DataFrame, span_indices: Tensor,\n",
        "                     labels2ids):\n",
        "    batch_size = len(batch_addresses[0])\n",
        "\n",
        "    span_number = span_indices.shape[0]\n",
        "\n",
        "    span_indices = span_indices.tolist()\n",
        "\n",
        "    batch_labels = np.ones((batch_size, span_number))*-100\n",
        "    batch_type = np.zeros((batch_size, span_number))\n",
        "\n",
        "    for batch_idx, i in enumerate(zip(batch_addresses[0], batch_addresses[1].detach().tolist())):\n",
        "        textid = i[0]\n",
        "        sentid = i[1]\n",
        "\n",
        "        labels_df = list(spans_dataset[spans_dataset['TextID']==textid][spans_dataset['SentID']==sentid]['Spans'])\n",
        "        readable_spans = []\n",
        "        for ent in labels_df:\n",
        "            temp = ent[1:-1].split(', ')\n",
        "            readable_spans.append([int(temp[0]), int(temp[1]),labels2ids[temp[2][1:-1]]])\n",
        "        \n",
        "        \n",
        "        true_indices = indices_dataset[indices_dataset['TextID']==textid][indices_dataset['SentID']==sentid]['Indices'].iloc[0]\n",
        "        max_ind = true_indices[-1][-1]\n",
        "        for idx, spn in enumerate(span_indices):\n",
        "            if spn[0] <= max_ind or spn[1] <= max_ind:\n",
        "                batch_labels[batch_idx][idx] = 0\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        for span in readable_spans:\n",
        "            temp_span_ids = []\n",
        "            memory = []\n",
        "            for idx, spn in enumerate(true_indices):\n",
        "                if spn[0] in range(span[0], span[1]+1) and spn[1] in range(span[0], span[1]+1):\n",
        "                    if temp_span_ids == []:\n",
        "                        temp_span_ids = [idx, idx]\n",
        "                        memory = [spn[0], spn[1]]\n",
        "                    else:\n",
        "                        temp_span_ids[-1] = idx\n",
        "                        memory[-1] = spn[1]\n",
        "            if temp_span_ids != []:\n",
        "                try:\n",
        "                    place = span_indices.index(temp_span_ids)\n",
        "                    batch_labels[batch_idx][place] = span[2]\n",
        "                    batch_type[batch_idx][place] = 1\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "    return torch.from_numpy(batch_labels).type(torch.int64), torch.from_numpy(batch_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF-XgZee1rXs"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "\n",
        "    def __init__(self, ids2labels):\n",
        "        _ = ids2labels.pop(0)\n",
        "        self.ids2labels = ids2labels\n",
        "        self.num_types = len(self.ids2labels)\n",
        "\n",
        "    def evaluate(self, y_true, y_pred):\n",
        "\n",
        "        # y_true, y_pred передаются батчами/списками\n",
        "\n",
        "        tp, fn, fp = 0, 0, 0\n",
        "        sub_tp, sub_fn, sub_fp = [0] * self.num_types, [0] * self.num_types, [0] * self.num_types\n",
        "\n",
        "        for gold_example, pred_example in zip(y_true, y_pred):\n",
        "            gold_ners = set([(idx, int(ent)) for idx, ent in enumerate(gold_example) if ent != 0])\n",
        "            pred_ners = set([(idx, int(ent)) for idx, ent in enumerate(pred_example) if ent != 0])\n",
        "\n",
        "            tp += len(gold_ners & pred_ners)\n",
        "            fn += len(gold_ners - pred_ners)\n",
        "            fp += len(pred_ners - gold_ners)\n",
        "            for i in range(self.num_types):\n",
        "                sub_gm = set((idx, ent) for idx, ent in gold_ners if ent == i+1)\n",
        "                sub_pm = set((idx, ent) for idx, ent in pred_ners if ent == i+1)\n",
        "                sub_tp[i] += len(sub_gm & sub_pm)\n",
        "                sub_fn[i] += len(sub_gm - sub_pm)\n",
        "                sub_fp[i] += len(sub_pm - sub_gm)\n",
        "\n",
        "        m_r = 0 if tp == 0 else float(tp) / (tp+fn)\n",
        "        m_p = 0 if tp == 0 else float(tp) / (tp+fp)\n",
        "        m_f1 = 0 if m_p == 0 else 2.0*m_r*m_p / (m_r+m_p)\n",
        "        logger.info(\"Mention F1: {:.5f}%\".format(m_f1 * 100))\n",
        "        logger.info(\"Mention Recall: {:.5f}%\".format(m_r * 100))\n",
        "        logger.info(\"Mention Precision: {:.5f}%\".format(m_p * 100))\n",
        "        logger.info(\"****************SUB NER TYPES********************\")\n",
        "        f1_scores_list = []\n",
        "        for i in range(self.num_types):\n",
        "            if i+1 not in [24, 28, 29]:\n",
        "                sub_r = 0 if sub_tp[i] == 0 else float(sub_tp[i]) / (sub_tp[i] + sub_fn[i])\n",
        "                sub_p = 0 if sub_tp[i] == 0 else float(sub_tp[i]) / (sub_tp[i] + sub_fp[i])\n",
        "                sub_f1 = 0 if sub_p == 0 else 2.0 * sub_r * sub_p / (sub_r + sub_p)\n",
        "                f1_scores_list.append(sub_f1)\n",
        "                logger.info(\"{} F1: {:.5f}%\".format(self.ids2labels[i+1], sub_f1 * 100))\n",
        "                logger.info(\"{} Recall: {:.5f}%\".format(self.ids2labels[i+1], sub_r * 100))\n",
        "                logger.info(\"{} Precision: {:.5f}%\".format(self.ids2labels[i+1], sub_p * 100))\n",
        "                logger.info(f'{sub_tp[i]}, {sub_fn[i]}, {sub_fp[i]}')\n",
        "        summary_dict = {}\n",
        "        summary_dict[\"Mention F1\"] = m_f1\n",
        "        summary_dict[\"Mention Recall\"] = m_r\n",
        "        summary_dict[\"Mention Precision\"] = m_p\n",
        "\n",
        "        summary_dict[\"Macro F1\"] = sum([each for i, each in enumerate(f1_scores_list)]) \\\n",
        "            / float(self.num_types-3)\n",
        "        logger.info(\"Macro F1: {:.5f}%\".format(summary_dict[\"Macro F1\"] * 100))\n",
        "        return summary_dict[\"Macro F1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhgO4hgFR_OO"
      },
      "source": [
        "# SpanExtraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQAJ2IxM_v29"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV5-VMYeOlZo"
      },
      "outputs": [],
      "source": [
        "class ConfigurationError(Exception):\n",
        "    \"\"\"\n",
        "    The exception raised by any AllenNLP object when it's misconfigured\n",
        "    (e.g. missing properties, invalid properties, unknown properties).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, message: str):\n",
        "        super().__init__()\n",
        "        self.message = message\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.message\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVOonpo_3tl9"
      },
      "outputs": [],
      "source": [
        "def get_range_vector(size: int, device: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Возвращает вектор диапазона требуемого размера, начинающийся с 0.\n",
        "    \"\"\"\n",
        "    if device > -1:\n",
        "        return cuda.LongTensor(\n",
        "            size, device=device).fill_(1).cumsum(0) - 1\n",
        "    else:\n",
        "        return torch.arange(0, size, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWQNFvaD37R_"
      },
      "outputs": [],
      "source": [
        "def get_device_of(tensor: Tensor) -> int:\n",
        "    \"\"\"\n",
        "    Возвращает, на cuda или cpu требуемый тензор\n",
        "    \"\"\"\n",
        "    if not tensor.is_cuda:\n",
        "        return -1\n",
        "    else:\n",
        "        return tensor.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAOLLLnz6eO_"
      },
      "outputs": [],
      "source": [
        "def bucket_values(distances: torch.Tensor,\n",
        "                  num_identity_buckets: int=4,\n",
        "                  num_total_buckets: int=10) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Кластеризует значения в `num_total_buckets`, num_identity_buckets из которых принимают\n",
        "    одно значение, а не диапазон.\n",
        "    По умолчанию:\n",
        "    [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+].\n",
        "    Используется для кодирования длин спэга\n",
        "    # Parameters\n",
        "    distances : `torch.Tensor`, required.\n",
        "        A Tensor of any size, to be bucketed.\n",
        "    num_identity_buckets: `int`, optional (default = `4`).\n",
        "        The number of identity buckets (those only holding a single value).\n",
        "    num_total_buckets : `int`, (default = `10`)\n",
        "        The total number of buckets to bucket values into.\n",
        "    # Returns\n",
        "    `torch.Tensor`\n",
        "        A tensor of the same shape as the input, containing the indices of the buckets\n",
        "        the values were placed in.\n",
        "    \"\"\"\n",
        "    # Chunk the values into semi-logscale buckets using .floor().\n",
        "    # This is a semi-logscale bucketing because we divide by log(2) after taking the log.\n",
        "    # We do this to make the buckets more granular in the initial range, where we expect\n",
        "    # most values to fall. We then add (num_identity_buckets - 1) because we want these indices\n",
        "    # to start _after_ the fixed number of buckets which we specified would only hold single values.\n",
        "    logspace_index = (distances.float().log() / math.log(2)).floor().long() + (\n",
        "        num_identity_buckets - 1)\n",
        "    # create a mask for values which will go into single number buckets (i.e not a range).\n",
        "    use_identity_mask = (distances <= num_identity_buckets).long()\n",
        "    use_buckets_mask = 1 + (-1 * use_identity_mask)\n",
        "    # Use the original values if they are less than num_identity_buckets, otherwise\n",
        "    # use the logspace indices.\n",
        "    combined_index = use_identity_mask * distances + use_buckets_mask * logspace_index\n",
        "    # Clamp to put anything > num_total_buckets into the final bucket.\n",
        "    return combined_index.clamp(0, num_total_buckets - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgbfHD1d6IC7"
      },
      "outputs": [],
      "source": [
        "def flatten_and_batch_shift_indices(indices: torch.Tensor,\n",
        "                                    sequence_length: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вспомогательная функция для batched_index_select (см. далее)\n",
        "    На вход функция получает \"indices\" размерности (batch_size, d_1, ..., d_n)\n",
        "    приводит все к размерности: (batch_size, sequence_length, embedding_size)\n",
        "    ```python\n",
        "        indices = torch.ones([2,3], dtype=torch.long)\n",
        "        # Sequence length of the target tensor.\n",
        "        sequence_length = 10\n",
        "        shifted_indices = flatten_and_batch_shift_indices(indices, sequence_length)\n",
        "        # Indices into the second element in the batch are correctly shifted\n",
        "        # to take into account that the target tensor will be flattened before\n",
        "        # the indices are applied.\n",
        "        assert shifted_indices == [1, 1, 1, 11, 11, 11]\n",
        "    ```\n",
        "    # Parameters\n",
        "    indices : `torch.LongTensor`, required.\n",
        "    sequence_length : `int`, required.\n",
        "        The length of the sequence the indices index into.\n",
        "        This must be the second dimension of the tensor.\n",
        "    # Returns\n",
        "    offset_indices : `torch.LongTensor`\n",
        "    \"\"\"\n",
        "    # Shape: (batch_size)\n",
        "    if torch.max(indices) >= sequence_length or torch.min(indices) < 0:\n",
        "        raise ConfigurationError(\n",
        "            f\"All elements in indices should be in range (0, {sequence_length - 1})\"\n",
        "        )\n",
        "    \n",
        "    offsets = (get_range_vector(indices.size(0), get_device_of(indices)) *\n",
        "               sequence_length)\n",
        "    for _ in range(len(indices.shape) - 1):\n",
        "        offsets = offsets.unsqueeze(1)\n",
        "\n",
        "    # Shape: (batch_size, d_1, ..., d_n)\n",
        "    offset_indices = indices + offsets\n",
        "\n",
        "    # Shape: (batch_size * d_1 * ... * d_n)\n",
        "    offset_indices = offset_indices.reshape(-1)\n",
        "    return offset_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMUO58gV5ezT"
      },
      "outputs": [],
      "source": [
        "def batched_index_select(\n",
        "        target: torch.Tensor,\n",
        "        indices: torch.LongTensor,\n",
        "        flattened_indices: Optional[torch.LongTensor]=None, ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    На вход функция получает \"indices\" размерности (batch_size, d_1, ..., d_n). Они индексируются\n",
        "    в размерность последовательности (dim 2). Размерность таргета: \n",
        "    (batch_size, sequence_length, embedding_size)\n",
        "    Возвращает отобранные значения в таргете с опорой на полученные индексы,\n",
        "    размера (batch_size, d_1, ..., d_n, embedding_size).\n",
        "    \n",
        "    # Parameters\n",
        "    target : `torch.Tensor`, required.\n",
        "        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).\n",
        "        This is the tensor to be indexed.\n",
        "    indices : `torch.LongTensor`\n",
        "        A tensor of shape (batch_size, ...), where each element is an index into the\n",
        "        `sequence_length` dimension of the `target` tensor.\n",
        "    flattened_indices : `Optional[torch.Tensor]`, optional (default = `None`)\n",
        "        An optional tensor representing the result of calling `flatten_and_batch_shift_indices`\n",
        "        on `indices`. This is helpful in the case that the indices can be flattened once and\n",
        "        cached for many batch lookups.\n",
        "    # Returns\n",
        "    selected_targets : `torch.Tensor`\n",
        "        A tensor with shape [indices.shape, target.size(-1)] representing the embedded indices\n",
        "        extracted from the batch flattened target tensor.\n",
        "    \"\"\"\n",
        "    if flattened_indices is None:\n",
        "        # Shape: (batch_size * d_1 * ... * d_n)\n",
        "        flattened_indices = flatten_and_batch_shift_indices(indices,\n",
        "                                                            target.size(1))\n",
        "\n",
        "    # Shape: (batch_size * sequence_length, embedding_size)\n",
        "    flattened_target = target.reshape(-1, target.size(-1))\n",
        "\n",
        "    # Shape: (batch_size * d_1 * ... * d_n, embedding_size)\n",
        "    flattened_selected = flattened_target.index_select(0, flattened_indices)\n",
        "    selected_shape = list(indices.shape) + [target.size(-1)]\n",
        "    # Shape: (batch_size, d_1, ..., d_n, embedding_size)\n",
        "    selected_targets = flattened_selected.reshape(*selected_shape)\n",
        "    return selected_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxXPlv9xIfdw"
      },
      "outputs": [],
      "source": [
        "def get_lengths_from_binary_sequence_mask(\n",
        "        mask: torch.BoolTensor) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Вычисление длины последовательности в каждом батче с помощьью бинарной маски\n",
        "    # Parameters\n",
        "    mask : `torch.BoolTensor`, required.\n",
        "        A 2D binary mask of shape (batch_size, sequence_length) to\n",
        "        calculate the per-batch sequence lengths from.\n",
        "    # Returns\n",
        "    `torch.LongTensor`\n",
        "        A torch.LongTensor of shape (batch_size,) representing the lengths\n",
        "        of the sequences in the batch.\n",
        "    \"\"\"\n",
        "    return mask.sum(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeeYKhSH1Gw3"
      },
      "outputs": [],
      "source": [
        "def batched_span_select(target: torch.Tensor,\n",
        "                        spans: torch.LongTensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    На вход получает спэны размерности (batch_size, num_spans, 2), \n",
        "    индексируется в размерность последовательности (dim 2) таргета, который\n",
        "    представлен размером: (batch_size, sequence_length, embedding_size)\n",
        "    Возвращает сегментированные спэны в таргете с учетом полученных индексов:\n",
        "    Эмбеддинг спэна размерности (batch_size, num_spans, max_batch_span_width, embedding_size)\n",
        "    # Parameters\n",
        "    target : `torch.Tensor`, required.\n",
        "        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).\n",
        "        This is the tensor to be indexed.\n",
        "    indices : `torch.LongTensor`\n",
        "        A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end\n",
        "        indices (both inclusive) into the `sequence_length` dimension of the `target` tensor.\n",
        "    # Returns\n",
        "    span_embeddings : `torch.Tensor`\n",
        "        A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size)\n",
        "        representing the embedded spans extracted from the batch flattened target tensor.\n",
        "    span_mask: `torch.BoolTensor`\n",
        "        A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on\n",
        "        the returned span embeddings.\n",
        "    \"\"\"\n",
        "    # both of shape (batch_size, num_spans, 1)\n",
        "    span_starts, span_ends = spans.split(1, dim=-1)\n",
        "\n",
        "\n",
        "    # shape (batch_size, num_spans, 1)\n",
        "    # These span widths are off by 1, because the span ends are `inclusive`.\n",
        "    span_widths = span_ends - span_starts\n",
        "\n",
        "    # We need to know the maximum span width so we can\n",
        "    # generate indices to extract the spans from the sequence tensor.\n",
        "    # These indices will then get masked below, such that if the length\n",
        "    # of a given span is smaller than the max, the rest of the values\n",
        "    # are masked.\n",
        "    max_batch_span_width = span_widths.max().item() + 1\n",
        "\n",
        "    # Shape: (1, 1, max_batch_span_width)\n",
        "    max_span_range_indices = get_range_vector(\n",
        "                            max_batch_span_width, get_device_of(target)).reshape(1, 1, -1)\n",
        "    # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "    # This is a broadcasted comparison - for each span we are considering,\n",
        "    # we are creating a range vector of size max_span_width, but masking values\n",
        "    # which are greater than the actual length of the span.\n",
        "    #\n",
        "    # We're using <= here (and for the mask below) because the span ends are\n",
        "    # inclusive, so we want to include indices which are equal to span_widths rather\n",
        "    # than using it as a non-inclusive upper bound.\n",
        "    span_mask = max_span_range_indices <= span_widths #(...).float()\n",
        "    raw_span_indices = span_starts + max_span_range_indices\n",
        "    # span_ends - max_span_range_indices\n",
        "\n",
        "    # We also don't want to include span indices which greater than the sequence_length,\n",
        "    # which happens because some spans near the end of the sequence\n",
        "    # have a start index + max_batch_span_width > sequence_length, so we add this to the mask here.\n",
        "    span_mask = (span_mask & (raw_span_indices < target.size(1)) &\n",
        "                 (0 <= raw_span_indices)) # доп ограничение к оригиналу\n",
        "    span_indices = raw_span_indices * span_mask \n",
        "\n",
        "    # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n",
        "    # flatten & batch at once\n",
        "    span_embeddings = batched_index_select(target, span_indices)\n",
        "\n",
        "    return span_embeddings, span_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mFz9Lec-xc7"
      },
      "outputs": [],
      "source": [
        "def tiny_value_of_dtype(dtype: torch.dtype):\n",
        "    \"\"\"\n",
        "    Возвращает относительно маленькое значение данного типа данных, \n",
        "    что применяется во избежание ошибок с вычислением (деление на 0)\n",
        "    Поддерживает только типы с плавающей точкой\n",
        "    \"\"\"\n",
        "    if not dtype.is_floating_point:\n",
        "        raise TypeError(\"Only supports floating point dtypes.\")\n",
        "    if dtype == torch.float or dtype == torch.double:\n",
        "        return 1e-13\n",
        "    elif dtype == torch.half:\n",
        "        return 1e-4\n",
        "    else:\n",
        "        raise TypeError(\"Does not support dtype \" + str(dtype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLprlsr9_GZ4"
      },
      "outputs": [],
      "source": [
        "def info_value_of_dtype(dtype: torch.dtype):\n",
        "    \"\"\"\n",
        "    Returns the `finfo` or `iinfo` object of a given PyTorch data type. Does not allow torch.bool.\n",
        "    \"\"\"\n",
        "    if dtype == torch.bool:\n",
        "        raise TypeError(\"Does not support torch.bool\")\n",
        "    elif dtype.is_floating_point:\n",
        "        return torch.finfo(dtype)\n",
        "    else:\n",
        "        return torch.iinfo(dtype)\n",
        "\n",
        "def min_value_of_dtype(dtype: torch.dtype):\n",
        "    \"\"\"\n",
        "    Returns the minimum value of a given PyTorch data type. Does not allow torch.bool.\n",
        "    \"\"\"\n",
        "    return info_value_of_dtype(dtype).min\n",
        "\n",
        "\n",
        "def max_value_of_dtype(dtype: torch.dtype):\n",
        "    \"\"\"\n",
        "    Returns the maximum value of a given PyTorch data type. Does not allow torch.bool.\n",
        "    \"\"\"\n",
        "    return info_value_of_dtype(dtype).max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXVZMyj08uw0"
      },
      "outputs": [],
      "source": [
        "def masked_max(\n",
        "        vector: torch.Tensor,\n",
        "        mask: torch.BoolTensor,\n",
        "        dim: int,\n",
        "        keepdim: bool=False, ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Вычисление макс пулинга\n",
        "    # Parameters\n",
        "    vector : `torch.Tensor`\n",
        "        The vector to calculate max, assume unmasked parts are already zeros\n",
        "    mask : `torch.BoolTensor`\n",
        "        The mask of the vector. It must be broadcastable with vector.\n",
        "    dim : `int`\n",
        "        The dimension to calculate max\n",
        "    keepdim : `bool`\n",
        "        Whether to keep dimension\n",
        "    # Returns\n",
        "    `torch.Tensor`\n",
        "        A `torch.Tensor` of including the maximum values.\n",
        "    \"\"\"\n",
        "    replaced_vector = vector.masked_fill(~mask,\n",
        "                                         min_value_of_dtype(vector.dtype))\n",
        "    max_value, _ = replaced_vector.max(dim=dim, keepdim=keepdim)\n",
        "    return max_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3EUtypTxXYv"
      },
      "outputs": [],
      "source": [
        "def weighted_sum(matrix: torch.Tensor,\n",
        "                 attention: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    На вход получает матрицу векторов и множество весов для рядов в этой матрице\n",
        "    - вектор внимания. После этого возвращается взвешеная сумма рядов в матрице.\n",
        "    \"\"\"\n",
        "    # We'll special-case a few settings here, where there are efficient (but poorly-named)\n",
        "    # operations in pytorch that already do the computation we need.\n",
        "    if attention.ndim == 2 and matrix.ndim == 3:\n",
        "        return attention.unsqueeze(1).bmm(matrix).squeeze(1) # batch matrix-matrix product\n",
        "    if attention.ndim == 3 and matrix.ndim == 3:\n",
        "        return attention.bmm(matrix)\n",
        "    if matrix.ndim - 1 < attention.ndim:\n",
        "        expanded_size = list(matrix.shape)\n",
        "        for i in range(attention.ndim - matrix.ndim + 1):\n",
        "            matrix = matrix.unsqueeze(1)\n",
        "            expanded_size.insert(i + 1, attention.size(i + 1))\n",
        "        matrix = matrix.expand(*expanded_size)\n",
        "    intermediate = attention.unsqueeze(-1).expand_as(matrix) * matrix\n",
        "    return intermediate.sum(dim=-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLanRfrixY3H"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(\n",
        "        vector: torch.Tensor,\n",
        "        mask: torch.BoolTensor,\n",
        "        dim: int=-1,\n",
        "        memory_efficient: bool=False, ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    F.softmax(vector) не применяется, так как некоторые элементы вектора могут быть\n",
        "    замаскированы. Поэтому функция применяет операцию softmax только на незамаскированной\n",
        "    части вектора.\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        result = F.softmax(vector, dim=dim)\n",
        "    else:\n",
        "        while mask.ndim < vector.ndim:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        if not memory_efficient:\n",
        "            # To limit numerical errors from large vector elements outside the mask, \n",
        "            # we zero these out.\n",
        "            result = F.softmax(vector * mask, dim=dim)\n",
        "            result = result * mask\n",
        "            result = result / (result.sum(dim=dim, keepdim=True) +\n",
        "                               tiny_value_of_dtype(result.dtype))\n",
        "        else:\n",
        "            masked_vector = vector.masked_fill(\n",
        "                ~mask, min_value_of_dtype(vector.dtype))\n",
        "            result = F.softmax(masked_vector, dim=dim)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEw1CbsJNeMj"
      },
      "outputs": [],
      "source": [
        "def replace_masked_values(\n",
        "    tensor: torch.Tensor, mask: torch.BoolTensor, replace_with: float\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Replaces all masked values in `tensor` with `replace_with`.  `mask` must be broadcastable\n",
        "    to the same shape as `tensor`. We require that `tensor.dim() == mask.dim()`, as otherwise we\n",
        "    won't know which dimensions of the mask to unsqueeze.\n",
        "    This just does `tensor.masked_fill()`, except the pytorch method fills in things with a mask\n",
        "    value of 1, where we want the opposite.  You can do this in your own code with\n",
        "    `tensor.masked_fill(~mask, replace_with)`.\n",
        "    \"\"\"\n",
        "    if tensor.dim() != mask.dim():\n",
        "        raise ConfigurationError(\n",
        "            \"tensor.dim() (%d) != mask.dim() (%d)\" % (tensor.dim(), mask.dim())\n",
        "        )\n",
        "    return tensor.masked_fill(~mask, replace_with)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAb0ykX7g8Cp"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, mask=None, dropout=None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return p_attn\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeYGKPeFjRih"
      },
      "outputs": [],
      "source": [
        "class TimeDistributed(nn.Module):\n",
        "    \"\"\"\n",
        "    На вход получает данные размерности (batch_size, time_steps, [rest]) и Module,\n",
        "    принимающий на вход данные размерности (batch_size, [rest])\n",
        "    Модуль TimeDistributed меняет размер входных данных на (batch_size * time_steps, [rest]),\n",
        "    применяет трансформации из Module и трансформирует размерность обратно.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, module):\n",
        "        super().__init__()\n",
        "        self._module = module\n",
        "\n",
        "    def forward(self, *inputs, pass_through: List[str]=None, **kwargs):\n",
        "\n",
        "        pass_through = pass_through or []\n",
        "\n",
        "        # решейп вводной информации из inputs\n",
        "        reshaped_inputs = [\n",
        "            self._reshape_tensor(input_tensor) for input_tensor in inputs\n",
        "        ]\n",
        "\n",
        "        # Требуются данные для определения размера батча (batch_size) и time_steps.\n",
        "        # Либо тензор из *inputs, либо из *kwargs\n",
        "        some_input = None\n",
        "        if inputs:\n",
        "            some_input = inputs[-1]\n",
        "\n",
        "        reshaped_kwargs = {}\n",
        "        for key, value in kwargs.items():\n",
        "            if isinstance(value, torch.Tensor) and key not in pass_through:\n",
        "                if some_input is None:\n",
        "                    some_input = value\n",
        "\n",
        "                value = self._reshape_tensor(value)\n",
        "\n",
        "            reshaped_kwargs[key] = value\n",
        "\n",
        "        # применение модели к преобразованным данным\n",
        "        reshaped_outputs = self._module(*reshaped_inputs, **reshaped_kwargs)\n",
        "\n",
        "        if some_input is None:\n",
        "            raise RuntimeError(\"No input tensor to time-distribute\")\n",
        "\n",
        "        # Приводим вывод к нужной размерности:\n",
        "        # (batch_size, time_steps, **output_size)\n",
        "        new_size = some_input.shape[:2] + reshaped_outputs.shape[1:]\n",
        "        outputs = reshaped_outputs.contiguous().reshape(new_size)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_tensor(input_tensor):\n",
        "        input_size = input_tensor.shape\n",
        "        if len(input_size) <= 2:\n",
        "            raise RuntimeError(f\"No dimension to distribute: {input_size}\")\n",
        "        # Сведение batch_size and time_steps в единую ось, размерность:\n",
        "        # (batch_size * time_steps, **input_size).\n",
        "        squashed_shape = [-1] + list(input_size[2:])\n",
        "        return input_tensor.reshape(*squashed_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0WrXaIx1-Uo"
      },
      "source": [
        "### Biaffine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGQEfTDKLDao"
      },
      "outputs": [],
      "source": [
        "class Biaffine(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    На вход получает данные размерности (batch_size, seq_max_len, input_dim) - результаты применения BiLSTM\n",
        "    Модуль вычисляет матрицу зависимостей для каждого токена с каждым, возвращает усредненные вектора\n",
        "    биаффинного внимания размерности (batch_size, seq_max_len, dep_vec_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, dep_vec_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.dep_vec_dim = dep_vec_dim\n",
        "\n",
        "        self.U_1 = Parameter(torch.Tensor(input_dim, dep_vec_dim, input_dim))\n",
        "        self.U_2 = Parameter(torch.Tensor(2*input_dim, dep_vec_dim))\n",
        "        self.bias = Parameter(torch.zeros(dep_vec_dim))\n",
        "\n",
        "        nn.init.xavier_uniform_(self.U_1)\n",
        "        nn.init.xavier_uniform_(self.U_2)\n",
        "        nn.init.constant_(self.bias, 0.)\n",
        "\n",
        "    def forward(self, h_forward, h_backward):\n",
        "\n",
        "        seq_len = h_forward.shape[1]\n",
        "        batch_size = h_forward.shape[0]\n",
        "\n",
        "        #Hf.T*U1*Hb # U1 - h*r*h, h - Hf/Hb dim, r - dep_vec_dim\n",
        "        # batch x seq_len x seq_len x dep_vec_dim\n",
        "        left_part= torch.einsum('bxi,irj,byj->bxyr', h_forward, self.U_1, h_backward)\n",
        "        \n",
        "        # (Hf⊕Hb).T*U2 # U2 - 2h*r\n",
        "        hf = torch.unsqueeze(h_forward, dim=2)\n",
        "        hf = torch.tile(hf, (1, 1, h_backward.shape[-2], 1))\n",
        "        hb = torch.unsqueeze(h_backward, dim=1)\n",
        "        hb = torch.tile(hb, (1, h_forward.shape[-2], 1, 1))\n",
        "\n",
        "        concat_h = torch.concat((hf, hb), dim=-1)\n",
        "        right_part = torch.einsum(\"bxyd,do->bxyo\", concat_h, self.U_2)\n",
        "\n",
        "        # batch x seq_len x seq_len x dep_vec_dim\n",
        "        biaff_matrix = left_part + right_part + self.bias\n",
        "\n",
        "        # batch x seq_len x 1 x dep_vec_dim\n",
        "        dep_vectors = nn.AvgPool3d((1, seq_len, 1))(biaff_matrix)\n",
        "        # batch x seq_len x dep_vec_dim\n",
        "        dep_vectors = dep_vectors.view(batch_size, seq_len, self.dep_vec_dim)        \n",
        "\n",
        "        return dep_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "56JItmsiVEDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX7lFiBWQezY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_dim: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.query_weight = nn.Linear(n_dim, n_dim)\n",
        "        self.key_weight = nn.Linear(n_dim, n_dim, bias=False)\n",
        "        self.value_weight = nn.Linear(n_dim, n_dim)\n",
        "        self.linear = nn.Linear(n_dim, n_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sequence: Tensor,\n",
        "        mask: Optional[Tensor]=None\n",
        "    ):\n",
        "        query = self.query_weight(sequence)\n",
        "        key = self.key_weight(sequence)\n",
        "        value = self.value_weight(sequence)\n",
        "\n",
        "        wv, qk = self.compute_attention(query, key, value, mask)\n",
        "        return self.linear(wv), qk\n",
        "\n",
        "    def compute_attention(\n",
        "        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n",
        "    ):\n",
        "        n_batch, n_ctx, n_state = q.shape\n",
        "        scale = (n_state // self.n_heads) ** -0.25\n",
        "        q = q.view(*q.shape[:2], self.n_heads, -1).permute(0, 2, 1, 3) * scale\n",
        "        k = k.view(*k.shape[:2], self.n_heads, -1).permute(0, 2, 3, 1) * scale\n",
        "        v = v.view(*v.shape[:2], self.n_heads, -1).permute(0, 2, 1, 3)\n",
        "\n",
        "        qk = q @ k\n",
        "        if mask is not None:\n",
        "            qk = qk + mask[:n_ctx, :n_ctx]\n",
        "        qk = qk.float()\n",
        "\n",
        "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
        "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wtMgoszJIgQ"
      },
      "source": [
        "### AGGCN modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPwioD_zJSbe"
      },
      "outputs": [],
      "source": [
        "class GraphConvLayer(nn.Module):\n",
        "    \"\"\" A GCN module operated on dependency graphs. \"\"\"\n",
        "\n",
        "    def __init__(self, tree_dropout, mem_dim, layers):\n",
        "        super(GraphConvLayer, self).__init__()\n",
        "        self.mem_dim = mem_dim\n",
        "        self.layers = layers\n",
        "        self.head_dim = self.mem_dim // self.layers\n",
        "        self.gcn_drop = nn.Dropout(tree_dropout)\n",
        "\n",
        "        # linear transformation\n",
        "        self.linear_output = nn.Linear(self.mem_dim, self.mem_dim)\n",
        "\n",
        "        # dcgcn block\n",
        "        self.weight_list = nn.ModuleList()\n",
        "        for i in range(self.layers):\n",
        "            self.weight_list.append(nn.Linear((self.mem_dim + self.head_dim * i), self.head_dim))\n",
        "\n",
        "\n",
        "    def forward(self, adj, gcn_inputs):\n",
        "        # gcn layer\n",
        "        denom = adj.sum(2).unsqueeze(2) + 1\n",
        "\n",
        "        outputs = gcn_inputs\n",
        "        cache_list = [outputs]\n",
        "        outputs = outputs.type(torch.float64)\n",
        "        output_list = []\n",
        "        for l in range(self.layers):\n",
        "            Ax = adj.bmm(outputs)\n",
        "            Ax = Ax.type(torch.float32)\n",
        "            outputs = outputs.type(torch.float32)\n",
        "            AxW = self.weight_list[l](Ax)\n",
        "            AxW = AxW + self.weight_list[l](outputs)  # self loop\n",
        "            AxW = AxW / denom\n",
        "            \n",
        "            gAxW = F.relu(AxW)\n",
        "            cache_list.append(gAxW)\n",
        "            outputs = torch.cat(cache_list, dim=2)\n",
        "            output_list.append(self.gcn_drop(gAxW))\n",
        "\n",
        "        gcn_outputs = torch.cat(output_list, dim=2)\n",
        "        gcn_outputs = gcn_outputs + gcn_inputs\n",
        "        \n",
        "        gcn_outputs = gcn_outputs.type(torch.float32)\n",
        "        out = self.linear_output(gcn_outputs)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gYEFQIyKtgy"
      },
      "outputs": [],
      "source": [
        "class MultiGraphConvLayer(nn.Module):\n",
        "    \"\"\" A GCN module operated on dependency graphs. \"\"\"\n",
        "\n",
        "    def __init__(self, tree_dropout, mem_dim, layers, heads):\n",
        "        super(MultiGraphConvLayer, self).__init__()\n",
        "        self.mem_dim = mem_dim\n",
        "        self.layers = layers\n",
        "        self.head_dim = self.mem_dim // self.layers\n",
        "        self.heads = heads\n",
        "        self.gcn_drop = nn.Dropout(tree_dropout)\n",
        "\n",
        "        # dcgcn layer\n",
        "        self.Linear = nn.Linear(self.mem_dim * self.heads, self.mem_dim)\n",
        "        self.weight_list = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.heads):\n",
        "            for j in range(self.layers):\n",
        "                self.weight_list.append(nn.Linear(self.mem_dim + self.head_dim * j, self.head_dim))\n",
        "\n",
        "\n",
        "    def forward(self, adj_list, gcn_inputs):\n",
        "\n",
        "        multi_head_list = []\n",
        "        for i in range(self.heads):\n",
        "            adj = adj_list[i]\n",
        "            denom = adj.sum(2).unsqueeze(2) + 1\n",
        "            outputs = gcn_inputs\n",
        "            cache_list = [outputs]\n",
        "            output_list = []\n",
        "            for l in range(self.layers):\n",
        "                index = i * self.layers + l\n",
        "                Ax = adj.bmm(outputs)\n",
        "                AxW = self.weight_list[index](Ax)\n",
        "                AxW = AxW + self.weight_list[index](outputs)  # self loop\n",
        "                AxW = AxW / denom\n",
        "                gAxW = F.relu(AxW)\n",
        "                cache_list.append(gAxW)\n",
        "                outputs = torch.cat(cache_list, dim=2)\n",
        "                output_list.append(self.gcn_drop(gAxW))\n",
        "\n",
        "            gcn_ouputs = torch.cat(output_list, dim=2)\n",
        "            gcn_ouputs = gcn_ouputs + gcn_inputs\n",
        "\n",
        "            multi_head_list.append(gcn_ouputs)\n",
        "\n",
        "        final_output = torch.cat(multi_head_list, dim=2)\n",
        "        out = self.Linear(final_output)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2tByMMtJCqK"
      },
      "outputs": [],
      "source": [
        "class GraphMultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(GraphMultiHeadAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 2)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        query, key = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "                             for l, x in zip(self.linears, (query, key))]\n",
        "        attn = attention(query, key, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        return attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHuc5IgT8vPb"
      },
      "outputs": [],
      "source": [
        "class AGGCN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 span_emb_dim: int,\n",
        "                 feature_dim: int,\n",
        "                 tree_prop: int = 1,\n",
        "                 tree_dropout: float=0.0, \n",
        "                 aggcn_heads: int=4,\n",
        "                 aggcn_sublayer_first: int=2,\n",
        "                 aggcn_sublayer_second: int=4):\n",
        "        super(AGGCN, self).__init__()\n",
        "\n",
        "        self.in_dim = span_emb_dim\n",
        "        self.mem_dim = span_emb_dim\n",
        "\n",
        "        self.input_W_G = nn.Linear(self.in_dim, self.mem_dim)\n",
        "\n",
        "        self.num_layers = tree_prop\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.heads = aggcn_heads\n",
        "        self.sublayer_first = aggcn_sublayer_first\n",
        "        self.sublayer_second = aggcn_sublayer_second\n",
        "\n",
        "        # gcn layer\n",
        "        for i in range(self.num_layers):\n",
        "            if i == 0:\n",
        "                self.layers.append(GraphConvLayer(tree_dropout, self.mem_dim, self.sublayer_first))\n",
        "                self.layers.append(GraphConvLayer(tree_dropout, self.mem_dim, self.sublayer_second))\n",
        "            else:\n",
        "                self.layers.append(MultiGraphConvLayer(tree_dropout, self.mem_dim, self.sublayer_first, self.heads))\n",
        "                self.layers.append(MultiGraphConvLayer(tree_dropout, self.mem_dim, self.sublayer_second, self.heads))\n",
        "\n",
        "        self.aggregate_W = nn.Linear(len(self.layers) * self.mem_dim, self.mem_dim)\n",
        "\n",
        "        self.attn = GraphMultiHeadAttention(self.heads, self.mem_dim)\n",
        "\n",
        "        # mlp output layer\n",
        "        in_dim = span_emb_dim\n",
        "        mlp_layers = [nn.Linear(in_dim, feature_dim), nn.ReLU()]\n",
        "        self.out_mlp = nn.Sequential(*mlp_layers)\n",
        "\n",
        "    # adj: (batch, sequence, sequence)\n",
        "    # text_embeddings: (batch, sequence, emb_dim)\n",
        "    # text_mask: (batch, sequence)\n",
        "    def forward(self, adj, text_embeddings, text_mask):\n",
        "\n",
        "        gcn_inputs = self.input_W_G(text_embeddings)\n",
        "        text_mask = text_mask.unsqueeze(-2)\n",
        "        layer_list = []\n",
        "        outputs = gcn_inputs\n",
        "        mask = (adj.sum(2) + adj.sum(1)).eq(0).unsqueeze(2)\n",
        "        for i in range(len(self.layers)):\n",
        "            if i < 2:\n",
        "                outputs = self.layers[i](adj, outputs)\n",
        "                layer_list.append(outputs)\n",
        "            else:\n",
        "                attn_tensor = self.attn(outputs, outputs, text_mask)\n",
        "                attn_adj_list = [attn_adj.squeeze(1) for attn_adj in torch.split(attn_tensor, 1, dim=1)]\n",
        "                outputs = self.layers[i](attn_adj_list, outputs)\n",
        "                layer_list.append(outputs)\n",
        "\n",
        "        aggregate_out = torch.cat(layer_list, dim=2)\n",
        "        dcgcn_output = self.aggregate_W(aggregate_out)\n",
        "\n",
        "        outputs = self.out_mlp(dcgcn_output)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aick26W9_6Rr"
      },
      "source": [
        "### Extractors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R51_c98ocUTS"
      },
      "outputs": [],
      "source": [
        "class SpanExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Many NLP models deal with representations of spans inside a sentence.\n",
        "    SpanExtractors define methods for extracting and representing spans\n",
        "    from a sentence.\n",
        "    SpanExtractors take a sequence tensor of shape (batch_size, timesteps, embedding_dim)\n",
        "    and indices of shape (batch_size, num_spans, 2) and return a tensor of\n",
        "    shape (batch_size, num_spans, ...), forming some representation of the\n",
        "    spans.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            sequence_mask: torch.BoolTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None, ):\n",
        "        \"\"\"\n",
        "        Given a sequence tensor, extract spans and return representations of\n",
        "        them. Span representation can be computed in many different ways,\n",
        "        such as concatenation of the start and end spans, attention over the\n",
        "        vectors contained inside the span, etc.\n",
        "        # Parameters\n",
        "        sequence_tensor : `torch.FloatTensor`, required.\n",
        "            A tensor of shape (batch_size, sequence_length, embedding_size)\n",
        "            representing an embedded sequence of words.\n",
        "        span_indices : `torch.LongTensor`, required.\n",
        "            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n",
        "            dimension represents the inclusive start and end indices of the\n",
        "            span to be extracted from the `sequence_tensor`.\n",
        "        sequence_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, sequence_length) representing padded\n",
        "            elements of the sequence.\n",
        "        span_indices_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, num_spans) representing the valid\n",
        "            spans in the `indices` tensor. This mask is optional because\n",
        "            sometimes it's easier to worry about masking after calling this\n",
        "            function, rather than passing a mask directly.\n",
        "        # Returns\n",
        "        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n",
        "        where `embedded_span_size` depends on the way spans are represented.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_input_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the expected final dimension of the `sequence_tensor`.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_output_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the expected final dimension of the returned span representation.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe6W0HCxSE53"
      },
      "outputs": [],
      "source": [
        "class MaxPoolingSpanExtractor(SpanExtractor):\n",
        "    \"\"\"\n",
        "    Представляет спэны с помощью макс-пулинга.\n",
        "    При заданном спэне x_i, ..., x_j, где i, j - начало и конец спэна,\n",
        "    каждое измерение d результирующего спэна вычисляется как s_d = max(x_id, ..., x_jd)\n",
        "    Registered as a `SpanExtractor` with name \"max_pooling\".\n",
        "    # Parameters\n",
        "    input_dim : `int`, required.\n",
        "        The final dimension of the `sequence_tensor`.\n",
        "    num_width_embeddings : `int`, optional (default = `None`).\n",
        "        Specifies the number of buckets to use when representing\n",
        "        span width features.\n",
        "    span_width_embedding_dim : `int`, optional (default = `None`).\n",
        "        The embedding size for the span_width features.\n",
        "    bucket_widths : `bool`, optional (default = `False`).\n",
        "        Whether to bucket the span widths into log-space buckets. If `False`,\n",
        "        the raw span widths are used.\n",
        "    # Returns\n",
        "    span_embeddings : `torch.FloatTensor`.\n",
        "    A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n",
        "    where `embedded_span_size` depends on the way spans are represented.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim: int,\n",
        "            num_width_embeddings: int=None,\n",
        "            span_width_embedding_dim: int=None,\n",
        "            bucket_widths: bool=False, ) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self._input_dim = input_dim\n",
        "        self._num_width_embeddings = num_width_embeddings\n",
        "        self._bucket_widths = bucket_widths\n",
        "\n",
        "        if num_width_embeddings is not None and span_width_embedding_dim is not None:\n",
        "            self._span_width_embedding = nn.Embedding(\n",
        "                                         num_embeddings=num_width_embeddings,\n",
        "                                         embedding_dim=span_width_embedding_dim)\n",
        "        elif num_width_embeddings is not None or span_width_embedding_dim is not None:\n",
        "            raise ConfigurationError(\n",
        "                \"To use a span width embedding representation, you must\"\n",
        "                \"specify both num_width_embeddings and span_width_embedding_dim.\"\n",
        "            )\n",
        "\n",
        "    def get_input_dim(self) -> int:\n",
        "        return self._input_dim\n",
        "\n",
        "    def get_output_dim(self) -> int:\n",
        "        if self._span_width_embedding is not None:\n",
        "            return self._input_dim + self._span_width_embedding.get_output_dim(\n",
        "            )\n",
        "        return self._input_dim\n",
        "\n",
        "    def _embed_spans(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            sequence_mask: torch.BoolTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None, ) -> torch.FloatTensor:\n",
        "\n",
        "        if sequence_tensor.size(-1) != self._input_dim:\n",
        "            raise ValueError(\n",
        "                f\"Dimension mismatch expected ({sequence_tensor.size(-1)}) \"\n",
        "                f\"received ({self._input_dim}).\")\n",
        "        if sequence_tensor.shape[1] <= span_indices.max() or span_indices.min(\n",
        "        ) < 0:\n",
        "            raise IndexError(\n",
        "                f\"Span index out of range, max index ({span_indices.max()}) \"\n",
        "                f\"or min index ({span_indices.min()}) \"\n",
        "                f\"not valid for sequence of length ({sequence_tensor.shape[1]}).\"\n",
        "            )\n",
        "\n",
        "        if (span_indices[:, :, 0] > span_indices[:, :, 1]).any():\n",
        "            raise IndexError(\"Span start above span end\", )\n",
        "\n",
        "\n",
        "        # Calculate the maximum sequence length for each element in batch.\n",
        "        # If span_end indices are above these length, we adjust the indices in adapted_span_indices\n",
        "        # проверка: для каждого элемента в батче вычисляется длина последовательности\n",
        "        # если индекс конца спэна выше этой длины, то индексы адаптируются\n",
        "        if sequence_mask is not None:\n",
        "            # shape (batch_size)\n",
        "            sequence_lengths = get_lengths_from_binary_sequence_mask(\n",
        "                sequence_mask)\n",
        "        else:\n",
        "            # shape (batch_size), filled with the sequence length size of the sequence_tensor.\n",
        "            sequence_lengths = torch.ones_like(\n",
        "                sequence_tensor[:, 0, 0],\n",
        "                dtype=torch.long) * sequence_tensor.size(1)\n",
        "\n",
        "        adapted_span_indices = torch.tensor(\n",
        "            span_indices, device=span_indices.device)\n",
        "\n",
        "        # простой циклический проход по батчам, если текущий конечный индекс больше или равен\n",
        "        # длине последовательности, то заменяем на макс_длину минус один\n",
        "        for b in range(sequence_lengths.shape[0]):\n",
        "            adapted_span_indices[b, :, 1][adapted_span_indices[b, :, 1] >=\n",
        "                                          sequence_lengths[b]] = (\n",
        "                                              sequence_lengths[b] - 1)\n",
        "\n",
        "        # Raise Error if span indices were completely masked by sequence mask.\n",
        "        # We only adjust span_end to the last valid index, so if span_end is below span_start,\n",
        "        # both were above the max index:\n",
        "        if (adapted_span_indices[:, :, 0] > adapted_span_indices[:, :, 1]\n",
        "            ).any():\n",
        "            raise IndexError(\n",
        "                \"Span indices were masked out entirely by sequence mask\", )\n",
        "\n",
        "        # span_vals <- (batch x num_spans x max_span_length x dim)\n",
        "        span_vals, span_mask = batched_span_select(sequence_tensor,\n",
        "                                                   adapted_span_indices)\n",
        "\n",
        "        # The application of masked_max requires a mask of the same shape as span_vals\n",
        "        # We repeat the mask along the last dimension (embedding dimension)\n",
        "        repeat_dim = len(span_vals.shape) - 1\n",
        "        repeat_idx = [1] * (repeat_dim) + [span_vals.shape[-1]]\n",
        "\n",
        "        # Shape: (batch x num_spans x max_span_length x dim)\n",
        "        # ext_span_mask True for values in span, False for masked out values\n",
        "        ext_span_mask = span_mask.unsqueeze(repeat_dim).repeat(repeat_idx)\n",
        "\n",
        "        # Shape: (batch x num_spans x embedding_dim)\n",
        "        max_output = masked_max(span_vals, ext_span_mask, dim=-2)\n",
        "\n",
        "        return max_output\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            sequence_mask: torch.BoolTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None, ):\n",
        "        \"\"\"\n",
        "        Функция для извлечение спэнов, получения семантического эмбеддинга и конкатенации\n",
        "        с эмбеддингом по длине\n",
        "        # Parameters\n",
        "        sequence_tensor : `torch.FloatTensor`, required.\n",
        "            A tensor of shape (batch_size, sequence_length, embedding_size)\n",
        "            representing an embedded sequence of words.\n",
        "        span_indices : `torch.LongTensor`, required.\n",
        "            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n",
        "            dimension represents the inclusive start and end indices of the\n",
        "            span to be extracted from the `sequence_tensor`.\n",
        "        sequence_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, sequence_length) representing padded\n",
        "            elements of the sequence.\n",
        "        span_indices_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, num_spans) representing the valid\n",
        "            spans in the `indices` tensor. This mask is optional because\n",
        "            sometimes it's easier to worry about masking after calling this\n",
        "            function, rather than passing a mask directly.\n",
        "        # Returns\n",
        "        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n",
        "        where `embedded_span_size` depends on the way spans are represented.\n",
        "        \"\"\"\n",
        "        # shape (batch_size, num_spans, embedding_dim)\n",
        "        span_embeddings = self._embed_spans(sequence_tensor, span_indices,\n",
        "                                            sequence_mask, span_indices_mask)\n",
        "        if self._span_width_embedding is not None:\n",
        "            # width = end_index - start_index + 1 since `SpanField` use inclusive indices.\n",
        "            # But here we do not add 1 because we often initiate the span width\n",
        "            # embedding matrix with `num_width_embeddings = max_span_width`\n",
        "            # shape (batch_size, num_spans)\n",
        "            widths_minus_one = span_indices[..., 1] - span_indices[..., 0]\n",
        "\n",
        "            if self._bucket_widths:\n",
        "                widths_minus_one = bucket_values(\n",
        "                    widths_minus_one,\n",
        "                    num_total_buckets=self._num_width_embeddings)  # type: ignore\n",
        "\n",
        "            # Embed the span widths and concatenate to the rest of the representations.\n",
        "            span_width_embeddings = self._span_width_embedding(\n",
        "                widths_minus_one)\n",
        "            span_embeddings = torch.cat(\n",
        "                [span_embeddings, span_width_embeddings], -1)\n",
        "\n",
        "        if span_indices_mask is not None:\n",
        "            # Here we are masking the spans which were originally passed in as padding.\n",
        "            return span_embeddings * span_indices_mask.unsqueeze(-1)\n",
        "\n",
        "        return span_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq1bWc4vdtcs"
      },
      "outputs": [],
      "source": [
        "class SelfAttentiveSpanExtractor(SpanExtractor):\n",
        "    \"\"\"\n",
        "    Вычисляет представления спэнов с помощью генерации оценки внимания для каждого слова\n",
        "    в тексте. Представления спэнов вычисляются с учетом этих оценок нормализацией полученных \n",
        "    значений для слов внутри спэна.\n",
        "    Registered as a `SpanExtractor` with name \"self_attentive\".\n",
        "    # Parameters\n",
        "    input_dim : `int`, required.\n",
        "        The final dimension of the `sequence_tensor`.\n",
        "    num_width_embeddings : `int`, optional (default = `None`).\n",
        "        Specifies the number of buckets to use when representing\n",
        "        span width features.\n",
        "    span_width_embedding_dim : `int`, optional (default = `None`).\n",
        "        The embedding size for the span_width features.\n",
        "    bucket_widths : `bool`, optional (default = `False`).\n",
        "        Whether to bucket the span widths into log-space buckets. If `False`,\n",
        "        the raw span widths are used.\n",
        "    # Returns\n",
        "    attended_text_embeddings : `torch.FloatTensor`.\n",
        "        A tensor of shape (batch_size, num_spans, input_dim), which each span representation\n",
        "        is formed by locally normalising a global attention over the sequence. The only way\n",
        "        in which the attention distribution differs over different spans is in the set of words\n",
        "        over which they are normalized.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim: int,\n",
        "            reduced_dim: int,\n",
        "            num_width_embeddings: int=None,\n",
        "            span_width_embedding_dim: int=None,\n",
        "            bucket_widths: bool=False, ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._input_dim = input_dim\n",
        "        self._num_width_embeddings = num_width_embeddings\n",
        "        self._bucket_widths = bucket_widths\n",
        "        self._heads = 4\n",
        "        \n",
        "        if num_width_embeddings is not None and span_width_embedding_dim is not None:\n",
        "            self._span_width_embedding = nn.Embedding(\n",
        "                                         num_embeddings=num_width_embeddings,\n",
        "                                         embedding_dim=span_width_embedding_dim)\n",
        "        elif num_width_embeddings is not None or span_width_embedding_dim is not None:\n",
        "            raise ConfigurationError(\n",
        "                \"To use a span width embedding representation, you must\"\n",
        "                \"specify both num_width_embeddings and span_width_embedding_dim.\"\n",
        "            )\n",
        "\n",
        "        self.attn = MultiHeadAttention(self._input_dim, self._heads)\n",
        "        self.attn_dropout = nn.Dropout(0.2)\n",
        "        self.dim_reducer = nn.Linear(self._input_dim+self._input_dim, reduced_dim+1)\n",
        "        #self.dim_reducer = nn.LSTM(self._input_dim, reduced_dim+1,\n",
        "        #                           num_layers=1, bidirectional=False, batch_first=True)\n",
        "    \n",
        "    def _embed_spans(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            sequence_mask: torch.BoolTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None, ) -> torch.FloatTensor:\n",
        "        \n",
        "        attention_output, _ = self.attn(sequence_tensor)\n",
        "        attention_output = self.attn_dropout(attention_output)\n",
        "        \n",
        "        # shape (batch_size, sequence_length, embedding_dim + 1)\n",
        "        concat_tensor = torch.cat([sequence_tensor, attention_output],\n",
        "                                  -1)\n",
        "        concat_tensor = sequence_tensor + attention_output\n",
        "        #reduced_tensor, (_, _) = self.dim_reducer(concat_tensor)\n",
        "\n",
        "        concat_output, span_mask = batched_span_select(reduced_tensor,\n",
        "                                                       span_indices)\n",
        "\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n",
        "        span_embeddings = concat_output[:, :, :, :-1]\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "        span_attention_logits = concat_output[:, :, :, -1]\n",
        "\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "        span_attention_weights = masked_softmax(span_attention_logits,\n",
        "                                                span_mask)\n",
        "\n",
        "        # Do a weighted sum of the embedded spans with\n",
        "        # respect to the normalised attention distributions.\n",
        "        # Shape: (batch_size, num_spans, embedding_dim)\n",
        "        attended_text_embeddings = weighted_sum(span_embeddings,\n",
        "                                                span_attention_weights)\n",
        "\n",
        "        return attended_text_embeddings\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            sequence_mask: torch.BoolTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None, ):\n",
        "        \"\"\"\n",
        "        Функция для извлечение спэнов, получения семантического эмбеддинга и конкатенации\n",
        "        с эмбеддингом по длине\n",
        "        # Parameters\n",
        "        sequence_tensor : `torch.FloatTensor`, required.\n",
        "            A tensor of shape (batch_size, sequence_length, embedding_size)\n",
        "            representing an embedded sequence of words.\n",
        "        span_indices : `torch.LongTensor`, required.\n",
        "            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n",
        "            dimension represents the inclusive start and end indices of the\n",
        "            span to be extracted from the `sequence_tensor`.\n",
        "        sequence_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, sequence_length) representing padded\n",
        "            elements of the sequence.\n",
        "        span_indices_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, num_spans) representing the valid\n",
        "            spans in the `indices` tensor. This mask is optional because\n",
        "            sometimes it's easier to worry about masking after calling this\n",
        "            function, rather than passing a mask directly.\n",
        "        # Returns\n",
        "        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n",
        "        where `embedded_span_size` depends on the way spans are represented.\n",
        "        \"\"\"\n",
        "        # shape (batch_size, num_spans, embedding_dim)\n",
        "        span_embeddings = self._embed_spans(sequence_tensor, span_indices,\n",
        "                                            sequence_mask, span_indices_mask)\n",
        "        if self._span_width_embedding is not None:\n",
        "            # width = end_index - start_index + 1 since `SpanField` use inclusive indices.\n",
        "            # But here we do not add 1 beacuse we often initiate the span width\n",
        "            # embedding matrix with `num_width_embeddings = max_span_width`\n",
        "            # shape (batch_size, num_spans)\n",
        "            widths_minus_one = span_indices[..., 1] - span_indices[..., 0]\n",
        "\n",
        "            if self._bucket_widths:\n",
        "                widths_minus_one = bucket_values(\n",
        "                    widths_minus_one,\n",
        "                    num_total_buckets=self._num_width_embeddings)  # type: ignore\n",
        "\n",
        "            # Embed the span widths and concatenate to the rest of the representations.\n",
        "            span_width_embeddings = self._span_width_embedding(\n",
        "                widths_minus_one)\n",
        "            span_embeddings = torch.cat(\n",
        "                [span_embeddings, span_width_embeddings], -1)\n",
        "\n",
        "        if span_indices_mask is not None:\n",
        "            # Here we are masking the spans which were originally passed in as padding.\n",
        "            return span_embeddings * span_indices_mask.unsqueeze(-1)\n",
        "        return span_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEdfFbb5Qezc"
      },
      "outputs": [],
      "source": [
        "# биаффинный аттеншен c вниманием\n",
        "\n",
        "class SelfBiaffineSpanExtractor(SpanExtractor):\n",
        "    \n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim: int,\n",
        "            reduced_dim: int,\n",
        "            num_width_embeddings: int=None,\n",
        "            span_width_embedding_dim: int=None,\n",
        "            bucket_widths: bool=False, \n",
        "            use_gcn: bool=True,) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._input_dim = input_dim\n",
        "        self._num_width_embeddings = num_width_embeddings\n",
        "        self._bucket_widths = bucket_widths\n",
        "        self._use_gcn = use_gcn\n",
        "        self._span_width_embedding = None\n",
        "        self._heads = 4\n",
        "        \n",
        "        if num_width_embeddings is not None and span_width_embedding_dim is not None:\n",
        "            self._span_width_embedding = nn.Embedding(\n",
        "                                         num_embeddings=num_width_embeddings,\n",
        "                                         embedding_dim=span_width_embedding_dim)\n",
        "        elif num_width_embeddings is not None or span_width_embedding_dim is not None:\n",
        "            raise ConfigurationError(\n",
        "                \"To use a span width embedding representation, you must\"\n",
        "                \"specify both num_width_embeddings and span_width_embedding_dim.\"\n",
        "            )\n",
        "        self.attn = MultiHeadAttention(self._input_dim, self._heads)\n",
        "\n",
        "        self.lstm_dim = self._input_dim// 2 #768/2 обычно\n",
        "        # batch_first - (batch, seq, 2*dim)\n",
        "        self.bilstm = nn.LSTM(self._input_dim, self.lstm_dim, \n",
        "                         num_layers=1, bidirectional=True, batch_first=True)\n",
        "        #self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        self.dep_vec_dim = 256        \n",
        "        self.biaffine = Biaffine(self.lstm_dim, self.dep_vec_dim)\n",
        "\n",
        "        self.gcn_dim = 0\n",
        "        if self._use_gcn:\n",
        "            self.gcn_dim = 256\n",
        "            self.graph_module = AGGCN(self._input_dim, self.gcn_dim,\n",
        "                                tree_prop= 1,\n",
        "                                tree_dropout=0.2, \n",
        "                                aggcn_heads=4,\n",
        "                                aggcn_sublayer_first=2,\n",
        "                                aggcn_sublayer_second=4)\n",
        "\n",
        "        # embedding_dim+1\n",
        "        self.dim_reducer = nn.Linear(self._input_dim+self.dep_vec_dim+self.gcn_dim+self._input_dim, reduced_dim+1)\n",
        "    \n",
        "    def _embed_spans(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            adj_matrix: torch.FloatTensor=None,\n",
        "            sequence_mask:  torch.FloatTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None) -> torch.FloatTensor:\n",
        "        \n",
        "        attention_output, _ = self.attn(sequence_tensor)\n",
        "\n",
        "        output, _ = self.bilstm(sequence_tensor)\n",
        "        #output = self.dropout(F.leaky_relu(output))\n",
        "        h_forward = output[:, :, :self.lstm_dim]\n",
        "        h_backward = output[:, :, self.lstm_dim:]\n",
        "\n",
        "        dep_output = self.biaffine(h_forward, h_backward)\n",
        "        if self._use_gcn:\n",
        "            graph_output = self.graph_module(adj_matrix, sequence_tensor, sequence_mask)\n",
        "            concat_tensor = torch.cat((sequence_tensor, dep_output, graph_output, attention_output), -1)\n",
        "        else:\n",
        "            # batch x seq_len x (embedding_dim + 1) + dep_vec_dim\n",
        "            concat_tensor = torch.cat((sequence_tensor, dep_output, attention_output), -1)\n",
        "\n",
        "        reduced_tensor = self.dim_reducer(concat_tensor)\n",
        "        \n",
        "        concat_output, span_mask = batched_span_select(reduced_tensor,\n",
        "                                                       span_indices)\n",
        "\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n",
        "        span_embeddings = concat_output[:, :, :, :-1]\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "        span_attention_logits = concat_output[:, :, :, -1]\n",
        "\n",
        "\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "        span_attention_weights = masked_softmax(span_attention_logits,\n",
        "                                                span_mask)\n",
        "\n",
        "        # Do a weighted sum of the embedded spans with\n",
        "        # respect to the normalised attention distributions.\n",
        "        # Shape: (batch_size, num_spans, embedding_dim) # почему уменьшилась размерность\n",
        "\n",
        "        attended_text_embeddings = weighted_sum(span_embeddings,\n",
        "                                                span_attention_weights)\n",
        "        return attended_text_embeddings\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            adj_matrix: torch.FloatTensor=None,\n",
        "            sequence_mask: torch.FloatTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None):\n",
        "        \"\"\"\n",
        "        Функция для извлечение спэнов, получения семантического эмбеддинга и конкатенации\n",
        "        с эмбеддингом по длине\n",
        "        # Parameters\n",
        "        sequence_tensor : `torch.FloatTensor`, required.\n",
        "            A tensor of shape (batch_size, sequence_length, embedding_size)\n",
        "            representing an embedded sequence of words.\n",
        "        span_indices : `torch.LongTensor`, required.\n",
        "            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n",
        "            dimension represents the inclusive start and end indices of the\n",
        "            span to be extracted from the `sequence_tensor`.\n",
        "        sequence_mask : `torch.FloatTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, sequence_length) representing padded\n",
        "            elements of the sequence.\n",
        "        span_indices_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, num_spans) representing the valid\n",
        "            spans in the `indices` tensor. This mask is optional because\n",
        "            sometimes it's easier to worry about masking after calling this\n",
        "            function, rather than passing a mask directly.\n",
        "        # Returns\n",
        "        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n",
        "        where `embedded_span_size` depends on the way spans are represented.\n",
        "        \"\"\"\n",
        "        # shape (batch_size, num_spans, embedding_dim)\n",
        "        span_embeddings = self._embed_spans(sequence_tensor, span_indices, adj_matrix,\n",
        "                                            sequence_mask, span_indices_mask)\n",
        "        if self._span_width_embedding is not None:\n",
        "            # width = end_index - start_index + 1 since `SpanField` use inclusive indices.\n",
        "            # But here we do not add 1 beacuse we often initiate the span width\n",
        "            # embedding matrix with `num_width_embeddings = max_span_width`\n",
        "            # shape (batch_size, num_spans)\n",
        "            widths_minus_one = span_indices[..., 1] - span_indices[..., 0]\n",
        "\n",
        "            if self._bucket_widths:\n",
        "                widths_minus_one = bucket_values(\n",
        "                    widths_minus_one,\n",
        "                    num_total_buckets=self._num_width_embeddings)  # type: ignore\n",
        "\n",
        "            # Embed the span widths and concatenate to the rest of the representations.\n",
        "            span_width_embeddings = self._span_width_embedding(\n",
        "                widths_minus_one)\n",
        "            span_embeddings = torch.cat(\n",
        "                [span_embeddings, span_width_embeddings], -1)\n",
        "\n",
        "        if span_indices_mask is not None:\n",
        "            # Here we are masking the spans which were originally passed in as padding.\n",
        "            return span_embeddings * span_indices_mask.unsqueeze(-1)\n",
        "        return span_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PkI1UhzfYTI"
      },
      "outputs": [],
      "source": [
        "# биаффинный аттеншен\n",
        "\n",
        "class BiaffineSpanExtractor(SpanExtractor):\n",
        "    \"\"\"\n",
        "    Дополнительно кодирует каждое слово с помощью биаффинного внимания\n",
        "    В результате вектора зависимостей конкатенируются с семантическими и с \n",
        "    помощью макс пулинга получается представление спэнов.\n",
        "    Registered as a `SpanExtractor` with name \"self_attentive\".\n",
        "    # Parameters\n",
        "    input_dim : `int`, required.\n",
        "        The final dimension of the `sequence_tensor`.\n",
        "    num_width_embeddings : `int`, optional (default = `None`).\n",
        "        Specifies the number of buckets to use when representing\n",
        "        span width features.\n",
        "    span_width_embedding_dim : `int`, optional (default = `None`).\n",
        "        The embedding size for the span_width features.\n",
        "    bucket_widths : `bool`, optional (default = `False`).\n",
        "        Whether to bucket the span widths into log-space buckets. If `False`,\n",
        "        the raw span widths are used.\n",
        "    # Returns\n",
        "    attended_text_embeddings : `torch.FloatTensor`.\n",
        "        A tensor of shape (batch_size, num_spans, input_dim), which each span representation\n",
        "        is formed by locally normalising a global attention over the sequence. The only way\n",
        "        in which the attention distribution differs over different spans is in the set of words\n",
        "        over which they are normalized.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim: int,\n",
        "            reduced_dim: int,\n",
        "            num_width_embeddings: int=None,\n",
        "            span_width_embedding_dim: int=None,\n",
        "            bucket_widths: bool=False, \n",
        "            use_gcn: bool=True,) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._input_dim = input_dim\n",
        "        self._num_width_embeddings = num_width_embeddings\n",
        "        self._bucket_widths = bucket_widths\n",
        "        self._use_gcn = use_gcn\n",
        "        self._span_width_embedding = None\n",
        "        \n",
        "        if num_width_embeddings is not None and span_width_embedding_dim is not None:\n",
        "            self._span_width_embedding = nn.Embedding(\n",
        "                                         num_embeddings=num_width_embeddings,\n",
        "                                         embedding_dim=span_width_embedding_dim)\n",
        "        elif num_width_embeddings is not None or span_width_embedding_dim is not None:\n",
        "            raise ConfigurationError(\n",
        "                \"To use a span width embedding representation, you must\"\n",
        "                \"specify both num_width_embeddings and span_width_embedding_dim.\"\n",
        "            )\n",
        "        self.lstm_dim = self._input_dim// 2 #768/2 обычно\n",
        "        # batch_first - (batch, seq, 2*dim)\n",
        "        self.bilstm = nn.LSTM(self._input_dim, self.lstm_dim, \n",
        "                         num_layers=1, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        self.dep_vec_dim = 256        \n",
        "        self.biaffine = Biaffine(self.lstm_dim, self.dep_vec_dim)\n",
        "\n",
        "        self.gcn_dim = 0\n",
        "        if self._use_gcn:\n",
        "            self.gcn_dim = 256\n",
        "            self.graph_module = AGGCN(self._input_dim, self.gcn_dim,\n",
        "                                tree_prop= 1,\n",
        "                                tree_dropout=0.2, \n",
        "                                aggcn_heads=4,\n",
        "                                aggcn_sublayer_first=2,\n",
        "                                aggcn_sublayer_second=4)\n",
        "\n",
        "        # embedding_dim+1\n",
        "        self.dim_reducer = nn.Linear(768+256+self.gcn_dim, reduced_dim+1)\n",
        "    \n",
        "    def _embed_spans(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            adj_matrix: torch.FloatTensor=None,\n",
        "            sequence_mask:  torch.FloatTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None) -> torch.FloatTensor:\n",
        "        \n",
        "        output, _ = self.bilstm(sequence_tensor)\n",
        "        #output = self.dropout(F.leaky_relu(output))\n",
        "        h_forward = output[:, :, :self.lstm_dim]\n",
        "        h_backward = output[:, :, self.lstm_dim:]\n",
        "\n",
        "        dep_output = self.biaffine(h_forward, h_backward)\n",
        "        if self._use_gcn:\n",
        "            graph_output = self.graph_module(adj_matrix, sequence_tensor, sequence_mask)\n",
        "            concat_tensor = torch.cat((sequence_tensor, dep_output, graph_output), -1)\n",
        "        else:\n",
        "            # batch x seq_len x (embedding_dim + 1) + dep_vec_dim\n",
        "            concat_tensor = torch.cat((sequence_tensor, dep_output), -1)\n",
        "\n",
        "        reduced_tensor = self.dim_reducer(concat_tensor)\n",
        "        \n",
        "        concat_output, span_mask = batched_span_select(reduced_tensor,\n",
        "                                                       span_indices)\n",
        "\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n",
        "        span_embeddings = concat_output[:, :, :, :-1]\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "        span_attention_logits = concat_output[:, :, :, -1]\n",
        "\n",
        "\n",
        "        # Shape: (batch_size, num_spans, max_batch_span_width)\n",
        "        span_attention_weights = masked_softmax(span_attention_logits,\n",
        "                                                span_mask)\n",
        "\n",
        "        # Do a weighted sum of the embedded spans with\n",
        "        # respect to the normalised attention distributions.\n",
        "        # Shape: (batch_size, num_spans, embedding_dim) # почему уменьшилась размерность\n",
        "\n",
        "        attended_text_embeddings = weighted_sum(span_embeddings,\n",
        "                                                span_attention_weights)\n",
        "        return attended_text_embeddings\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            sequence_tensor: torch.FloatTensor,\n",
        "            span_indices: torch.LongTensor,\n",
        "            adj_matrix: torch.FloatTensor=None,\n",
        "            sequence_mask: torch.FloatTensor=None,\n",
        "            span_indices_mask: torch.BoolTensor=None):\n",
        "        \"\"\"\n",
        "        Функция для извлечение спэнов, получения семантического эмбеддинга и конкатенации\n",
        "        с эмбеддингом по длине\n",
        "        # Parameters\n",
        "        sequence_tensor : `torch.FloatTensor`, required.\n",
        "            A tensor of shape (batch_size, sequence_length, embedding_size)\n",
        "            representing an embedded sequence of words.\n",
        "        span_indices : `torch.LongTensor`, required.\n",
        "            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n",
        "            dimension represents the inclusive start and end indices of the\n",
        "            span to be extracted from the `sequence_tensor`.\n",
        "        sequence_mask : `torch.FloatTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, sequence_length) representing padded\n",
        "            elements of the sequence.\n",
        "        span_indices_mask : `torch.BoolTensor`, optional (default = `None`).\n",
        "            A tensor of shape (batch_size, num_spans) representing the valid\n",
        "            spans in the `indices` tensor. This mask is optional because\n",
        "            sometimes it's easier to worry about masking after calling this\n",
        "            function, rather than passing a mask directly.\n",
        "        # Returns\n",
        "        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n",
        "        where `embedded_span_size` depends on the way spans are represented.\n",
        "        \"\"\"\n",
        "        # shape (batch_size, num_spans, embedding_dim)\n",
        "        span_embeddings = self._embed_spans(sequence_tensor, span_indices, adj_matrix,\n",
        "                                            sequence_mask, span_indices_mask)\n",
        "        if self._span_width_embedding is not None:\n",
        "            # width = end_index - start_index + 1 since `SpanField` use inclusive indices.\n",
        "            # But here we do not add 1 beacuse we often initiate the span width\n",
        "            # embedding matrix with `num_width_embeddings = max_span_width`\n",
        "            # shape (batch_size, num_spans)\n",
        "            widths_minus_one = span_indices[..., 1] - span_indices[..., 0]\n",
        "\n",
        "            if self._bucket_widths:\n",
        "                widths_minus_one = bucket_values(\n",
        "                    widths_minus_one,\n",
        "                    num_total_buckets=self._num_width_embeddings)  # type: ignore\n",
        "\n",
        "            # Embed the span widths and concatenate to the rest of the representations.\n",
        "            span_width_embeddings = self._span_width_embedding(\n",
        "                widths_minus_one)\n",
        "            span_embeddings = torch.cat(\n",
        "                [span_embeddings, span_width_embeddings], -1)\n",
        "\n",
        "        if span_indices_mask is not None:\n",
        "            # Here we are masking the spans which were originally passed in as padding.\n",
        "            return span_embeddings * span_indices_mask.unsqueeze(-1)\n",
        "        return span_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtmiquRPwiUf"
      },
      "source": [
        "# NER Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4vH1iIQwlz2"
      },
      "outputs": [],
      "source": [
        "class NERTagger(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 num_labels: int,\n",
        "                 ff_dropout: float=0.4) -> None:\n",
        "        super(NERTagger, self).__init__()\n",
        "\n",
        "        self._num_labels = num_labels\n",
        "        feed_forward = nn.Sequential(nn.Linear(input_dim, 256), \n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(ff_dropout))\n",
        "        self._ner_scorer = torch.nn.Sequential(\n",
        "                           TimeDistributed(feed_forward),\n",
        "                           TimeDistributed(nn.Linear(256, self._num_labels-1)))\n",
        "    def forward(self,\n",
        "                spans: torch.IntTensor,\n",
        "                span_mask: torch.IntTensor,\n",
        "                span_embeddings: torch.IntTensor,\n",
        "                ner_labels: torch.IntTensor = None,\n",
        "                previous_step_output: Dict[str, Any] = None) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "\n",
        "        # Shape: (Batch size, Number of Spans, Span Embedding Size)\n",
        "        # span_embeddings\n",
        "        ner_scores = self._ner_scorer(span_embeddings)\n",
        "        # Give large negative scores to masked-out elements.\n",
        "        mask = span_mask.unsqueeze(-1)\n",
        "        ner_scores = replace_masked_values(ner_scores, mask, -1e20)\n",
        "        dummy_dims = [ner_scores.size(0), ner_scores.size(1), 1]\n",
        "        dummy_scores = ner_scores.new_zeros(*dummy_dims)\n",
        "        if previous_step_output is not None and \"predicted_span\" in previous_step_output and not self.training:\n",
        "            dummy_scores.masked_fill_(previous_step_output[\"predicted_span\"].bool().unsqueeze(-1), -1e20)\n",
        "            dummy_scores.masked_fill_((1-previous_step_output[\"predicted_span\"]).bool().unsqueeze(-1), 1e20)\n",
        "\n",
        "        ner_scores = torch.cat((dummy_scores, ner_scores), -1)\n",
        "\n",
        "        if previous_step_output is not None and \"predicted_seq_span\" in previous_step_output and not self.training:\n",
        "            for row_idx, all_spans in enumerate(spans):\n",
        "                pred_spans = previous_step_output[\"predicted_seq_span\"][row_idx]\n",
        "                pred_spans = all_spans.new_tensor(pred_spans)\n",
        "                for col_idx, span in enumerate(all_spans):\n",
        "                    if span_mask[row_idx][col_idx] == 0:\n",
        "                        continue\n",
        "                    bFind = False\n",
        "                    for pred_span in pred_spans:\n",
        "                        if span[0] == pred_span[0] and span[1] == pred_span[1]:\n",
        "                            bFind = True\n",
        "                            break\n",
        "                    if bFind:\n",
        "                        # if find, use the ner scores, set dummy to a big negative\n",
        "                        ner_scores[row_idx, col_idx, 0] = -1e20\n",
        "                    else:\n",
        "                        # if not find, use the previous step, set dummy to a big positive\n",
        "                        ner_scores[row_idx, col_idx, 0] = 1e20\n",
        "\n",
        "        return ner_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON1InxbBAkaj"
      },
      "source": [
        "# Main NNER Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRprnZHMHHCr"
      },
      "outputs": [],
      "source": [
        "class NNERModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_labels,\n",
        "                 dropout_rate=0.4,\n",
        "                 max_seq_len = 284,\n",
        "                 max_span_len=None,\n",
        "                 extractor_type='biaffine'):\n",
        "        ## Add Extractor Class\n",
        "        super(NNERModel, self).__init__()\n",
        "        \n",
        "        self.max_span_len = max_span_len\n",
        "        self.num_labels = num_labels\n",
        "        self.extractor_type = extractor_type\n",
        "\n",
        "        # init span_indices\n",
        "        if self.max_span_len == None or self.max_span_len > max_seq_len:\n",
        "            self.max_span_len = 50\n",
        "        elif self.max_span_len <= 0:\n",
        "            self.max_span_len = 50\n",
        "        self.max_span_len = int(self.max_span_len)\n",
        "\n",
        "        self.triangle_mask = (torch.triu(\n",
        "                        torch.ones(max_seq_len, max_seq_len), diagonal=0) - torch.triu(\n",
        "                        torch.ones(max_seq_len, max_seq_len), diagonal=max_span_len)).bool()\n",
        "        \n",
        "\n",
        "\n",
        "        self.encoder = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "        encoder_hidden_state = self.encoder.config.hidden_size\n",
        "\n",
        "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        reduced_dim = 512\n",
        "        width_dim = 128\n",
        "        if extractor_type=='biaffine':\n",
        "            self.extractor = BiaffineSpanExtractor(input_dim=encoder_hidden_state,\n",
        "                                                reduced_dim=reduced_dim,\n",
        "                                                num_width_embeddings = self.max_span_len,\n",
        "                                                span_width_embedding_dim=width_dim,\n",
        "                                                use_gcn = False)\n",
        "        elif extractor_type=='attention':\n",
        "            self.extractor = SelfAttentiveSpanExtractor(input_dim=encoder_hidden_state,\n",
        "                                                        reduced_dim=encoder_hidden_state,\n",
        "                                                        num_width_embeddings = self.max_span_len,\n",
        "                                                        span_width_embedding_dim=width_dim,\n",
        "                                                        )\n",
        "        elif extractor_type == 'selfbiaffine':\n",
        "            self.extractor = SelfBiaffineSpanExtractor(input_dim=encoder_hidden_state,\n",
        "                                                reduced_dim=reduced_dim,\n",
        "                                                num_width_embeddings = self.max_span_len,\n",
        "                                                span_width_embedding_dim=width_dim,\n",
        "                                                use_gcn = False)\n",
        "        elif extractor_type=='maxpooling':\n",
        "            pass\n",
        "        \n",
        "        self.pruner = nn.Linear(reduced_dim+width_dim, 1)\n",
        "        \n",
        "        self.classifier = NERTagger(reduced_dim+width_dim, self.num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        adj_matrix=None\n",
        "        ):\n",
        "\n",
        "        batch_size, max_seq_len = input_ids.size(0), input_ids.size(1)\n",
        "\n",
        "        embedded_text_input = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embedded_text_input = embedded_text_input.last_hidden_state\n",
        "        embedded_text_input = self.encoder_dropout(F.leaky_relu(embedded_text_input))\n",
        "\n",
        "        span_indices = self.triangle_mask.nonzero().unsqueeze(0).expand(\n",
        "                        batch_size, -1, -1).to(device)\n",
        "\n",
        "        if self.extractor_type in ['biaffine', 'selfbiaffine']:\n",
        "            span_embeddings = self.extractor(sequence_tensor=embedded_text_input,\n",
        "                                            span_indices=span_indices,\n",
        "                                            adj_matrix=adj_matrix,\n",
        "                                            sequence_mask=attention_mask)\n",
        "        else:\n",
        "            span_embeddings = self.extractor(sequence_tensor=embedded_text_input,\n",
        "                                            span_indices=span_indices,\n",
        "                                            sequence_mask=attention_mask)\n",
        "\n",
        "        typing = self.pruner(span_embeddings)\n",
        "        #span_mask = torch.ones((typing.shape[0], typing.shape[1])).bool().to(device)\n",
        "        span_mask = torch.argmax(typing, dim=2).bool() #0.6\n",
        "        #masked_span_embeddings = torch.mul(span_embeddings, type_mask.unsqueeze(-1))\n",
        "\n",
        "        output = self.classifier(spans=span_indices,\n",
        "                                span_mask=span_mask,\n",
        "                                span_embeddings=span_embeddings)\n",
        "        \n",
        "        \n",
        "        \n",
        "        return output, typing.reshape(typing.shape[0], typing.shape[1]), span_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpH02mFLI5bs"
      },
      "outputs": [],
      "source": [
        "with open('/home/data_v3/labels2ids.pkl', 'rb') as pkl:\n",
        "    labels2ids = pickle.load(pkl)\n",
        "\n",
        "with open('/home/data_v3/ids2labels.pkl', 'rb') as pkl_two:\n",
        "    ids2labels = pickle.load(pkl_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJheWM3Ni4cS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHVcgfe3yGZG"
      },
      "outputs": [],
      "source": [
        "ner_ds_path = '/home/data_v3/train_texts.csv'\n",
        "train_texts = pd.read_csv(ner_ds_path, sep=';')\n",
        "\n",
        "to_del = [idx for idx, sent in enumerate(train_texts.Contents) if re.match(r'^\\s+$', sent)]\n",
        "\n",
        "train_texts = train_texts.drop(to_del).reset_index()\n",
        "\n",
        "train_texts = train_texts.drop('index', axis=1)\n",
        "\n",
        "train_inds = get_ind_sequence(train_texts)\n",
        "\n",
        "# тренировочные лейблы\n",
        "ner_ds_path = '/home/data_v3/train_spans.csv'\n",
        "train_spans = pd.read_csv(ner_ds_path, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl5MNF_llbyr"
      },
      "outputs": [],
      "source": [
        "train_texts_ds = dataset(train_texts, max_len=284)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_wau_Pujm1c"
      },
      "source": [
        "val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G2AFvkWjori"
      },
      "outputs": [],
      "source": [
        "ner_ds_path = '/home/data_v3/dev_texts.csv'\n",
        "val_texts = pd.read_csv(ner_ds_path, sep=';')\n",
        "\n",
        "to_del = [idx for idx, sent in enumerate(val_texts.Contents) if re.match(r'^\\s+$', sent)]\n",
        "\n",
        "val_texts = val_texts.drop(to_del).reset_index()\n",
        "\n",
        "val_texts = val_texts.drop('index', axis=1)\n",
        "\n",
        "val_inds = get_ind_sequence(val_texts)\n",
        "\n",
        "ner_ds_path = '/home/data_v3/dev_spans.csv'\n",
        "val_spans = pd.read_csv(ner_ds_path, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdi-cKK7jooi"
      },
      "outputs": [],
      "source": [
        "val_texts_ds = dataset(val_texts, max_len=284)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNprNbIQqraY"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cnYpJMejoi-"
      },
      "outputs": [],
      "source": [
        "\n",
        "ner_ds_path = '/home/data_v3/test_texts.csv'\n",
        "test_texts = pd.read_csv(ner_ds_path, sep=';')\n",
        "\n",
        "to_del = [idx for idx, sent in enumerate(test_texts.Contents) if re.match(r'^\\s+$', sent)]\n",
        "\n",
        "test_texts = test_texts.drop(to_del).reset_index()\n",
        "\n",
        "test_texts = test_texts.drop('index', axis=1)\n",
        "\n",
        "test_inds = get_ind_sequence(test_texts)\n",
        "\n",
        "ner_ds_path = '/home/data_v3/test_spans.csv'\n",
        "test_spans = pd.read_csv(ner_ds_path, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGAlfsBbvW7p"
      },
      "outputs": [],
      "source": [
        "test_texts_ds = dataset(test_texts, max_len=284)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YBm9OJUmuju"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrW-qSVN4J4k"
      },
      "outputs": [],
      "source": [
        "TRAIN_BATCH_SIZE = 1\n",
        "VALID_BATCH_SIZE = 1\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "\n",
        "training_loader = DataLoader(train_texts_ds, **train_params)\n",
        "validation_loader = DataLoader(val_texts_ds, **val_params)\n",
        "testing_loader = DataLoader(test_texts_ds, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ND40jDVsBU"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "e7c3398925a2458997742c6c47538c24",
            "26d03f7f655442afb23738bb3107f0b2",
            "0cecf36c5906483a9dd288ffe365b4cf",
            "d5604096cec347d3ae35cffe92c7e36c",
            "55f81c6e7b8f40349c1c861da7537acf",
            "962a3eb864304d35ad50d6985e973334",
            "2330a69f1da749b3aa8caafb75452f4b",
            "798f2df6745945eb8e1f77860104af30",
            "19afba11c04b4dcba9b01322d9d1512d",
            "def9fcd1f79f44db86bfe38778597dcb",
            "8e2c1ca40e894658aa7d91cb7b11dc83"
          ]
        },
        "id": "5kDy8ooJ9c0b",
        "outputId": "3ffde13e-d581-4a55-c6ca-c24f62e6cde7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = NNERModel(num_labels=len(ids2labels), max_span_len=26, max_seq_len=284,\n",
        "                  extractor_type='biaffine') # attention\n",
        "_ = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w-g1wgYW5f3"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=LEARNING_RATE)\n",
        "\n",
        "total_steps = len(training_loader) * EPOCHS\n",
        "warmup_steps = int(total_steps * 0.05)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FCDN3AS5rKG"
      },
      "outputs": [],
      "source": [
        "safe_prefix = 'testing_version_num_35'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8cu6hZU5i0H"
      },
      "outputs": [],
      "source": [
        "from_start = False # True при обучении модели с самого начала"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66GEPce15k__"
      },
      "outputs": [],
      "source": [
        "if from_start == False:\n",
        "    checkpoint = torch.load(f'/home/data_v3/{safe_prefix}_checkpoint.pth.tar')        \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    resume_epoch = checkpoint['epoch']\n",
        "    last_best = checkpoint['f_score']\n",
        "else:\n",
        "    last_best = 0\n",
        "    resume_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrI2A_u6alSk"
      },
      "outputs": [],
      "source": [
        "class Trainer(nn.Module):\n",
        "\n",
        "    def __init__(self, model, optimizer, scheduler, ids2labels, labels2ids):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "        self.circle_loss = CircleLoss(m=0.25, gamma=64)\n",
        "        self.entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.num_classes = len(ids2labels)\n",
        "        self.labels2ids = labels2ids    \n",
        "\n",
        "        self.evaluator = Evaluator(ids2labels)\n",
        "\n",
        "    def forward(self, data_loader, labels_dataset, data_indices, mode='train'):\n",
        "            \n",
        "            self.predictions = []\n",
        "            self.true_labels = []\n",
        "            epoch_f_score = 0\n",
        "            epoch_loss = 0\n",
        "\n",
        "            grad_regulator = torch.no_grad\n",
        "\n",
        "            if mode == 'train':\n",
        "                self.model.train()\n",
        "                grad_regulator = torch.enable_grad\n",
        "            \n",
        "            else:\n",
        "                self.model.eval()\n",
        "\n",
        "            with grad_regulator():\n",
        "                for idx, batch in tqdm(enumerate(data_loader)):\n",
        "                        if mode == 'train':\n",
        "                            self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                        input_ids = batch['input_ids'].to(device)\n",
        "                        attention_mask = batch['mask'].to(device)\n",
        "                        adj = batch['adj'].to(device)\n",
        "\n",
        "                        predicted, ent_type, span_indices = self.model(input_ids=input_ids,\n",
        "                                                                    attention_mask=attention_mask,\n",
        "                                                                    adj_matrix=adj)\n",
        "\n",
        "                        # get batch labels\n",
        "                        batch_addresses = batch['address']\n",
        "\n",
        "                        class_labels, type_labels = get_batch_labels(batch_addresses,\n",
        "                                                                     labels_dataset,\n",
        "                                                                     data_indices,\n",
        "                                                                     span_indices[0],\n",
        "                                                                     self.labels2ids)\n",
        "                        class_labels = class_labels.to(device)\n",
        "                        type_labels = type_labels.to(device)\n",
        "\n",
        "                        self.true_labels += class_labels.tolist()\n",
        "\n",
        "                        abs_labels = predicted.max(2).indices# для f_score\n",
        "                        self.predictions += abs_labels.tolist() \n",
        "                        \n",
        "                        if mode == 'train':\n",
        "                            # преобразование для вычисления ошибки\n",
        "                            predicted = predicted.view(-1, self.num_classes)\n",
        "                            class_labels = class_labels.view(-1)\n",
        "                            ent_type = ent_type.view(-1)\n",
        "                            type_labels = type_labels.view(-1)\n",
        "\n",
        "                            active_tokens = class_labels.view(-1) != -100\n",
        "                            class_labels = class_labels[active_tokens==1]\n",
        "                            predicted = predicted[active_tokens==1]\n",
        "                            ent_type = ent_type[active_tokens==1]\n",
        "                            type_labels = type_labels[active_tokens==1]\n",
        "\n",
        "                            # тест ce\n",
        "                            #ce_loss = self.entropy(predicted, class_labels)\n",
        "                            #loss = ce_loss\n",
        "\n",
        "                            # circle loss\n",
        "                            norm_preds = nn.functional.normalize(predicted)\n",
        "                            inp_sp, inp_sn = convert_label_to_similarity(norm_preds, class_labels)\n",
        "                            ccl_loss = self.circle_loss(inp_sp, inp_sn)\n",
        "                            loss = ccl_loss\n",
        "                                \n",
        "                            # loss for type (ent\\not-ent)\n",
        "                            bcl_loss = self.bce_loss(ent_type, type_labels)\n",
        "                            loss += bcl_loss\n",
        "\n",
        "                            epoch_loss += loss.item()\n",
        "                                \n",
        "                        if mode == 'train':\n",
        "                            loss.backward()\n",
        "                            self.optimizer.step()\n",
        "                            self.scheduler.step()\n",
        "                        \n",
        "                logger.info(f'RESULTS FOR MODE {mode.upper()}')\n",
        "                epoch_f_score = self.evaluator.evaluate(self.true_labels, self.predictions)\n",
        "                if mode == 'train':\n",
        "                    epoch_loss = epoch_loss / len(data_loader)\n",
        "                    logger.info(f'Loss per epoch: {epoch_loss}')\n",
        "            \n",
        "            return epoch_f_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1az8ttUwXpp"
      },
      "outputs": [],
      "source": [
        "nner_trainer = Trainer(model=model,\n",
        "                       optimizer=optimizer,\n",
        "                       scheduler=scheduler,\n",
        "                       ids2labels=ids2labels,\n",
        "                       labels2ids=labels2ids\n",
        "                       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLbzkZru22Xf"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    if resume_epoch+epoch+1 > EPOCHS:\n",
        "        break\n",
        "    logger.info(f'EPOCH {resume_epoch+epoch+1}/{EPOCHS}')\n",
        "    _ = nner_trainer(training_loader, train_spans, train_inds, mode='train')\n",
        "    logger.info('\\n')\n",
        "    f_score = nner_trainer(validation_loader, val_spans, val_inds, mode='dev')\n",
        "    logger.info('\\n')\n",
        "    f_score = nner_trainer(testing_loader, test_spans, test_inds, mode='test')\n",
        "\n",
        "    if f_score > last_best:\n",
        "        last_best = f_score\n",
        "        check_path = f'/home/data_v3/{safe_prefix}_checkpoint.pth.tar'\n",
        "        torch.save({'epoch': epoch+1+resume_epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        'f_score' : last_best}, check_path)\n",
        "    logger.info('\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ORZWMii99_j8",
        "XWiAByyWsf4o",
        "ERn8KXms1rtk",
        "uQAJ2IxM_v29",
        "T0WrXaIx1-Uo",
        "56JItmsiVEDT",
        "9wtMgoszJIgQ"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cecf36c5906483a9dd288ffe365b4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_798f2df6745945eb8e1f77860104af30",
            "max": 714355318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19afba11c04b4dcba9b01322d9d1512d",
            "value": 714355318
          }
        },
        "19afba11c04b4dcba9b01322d9d1512d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2330a69f1da749b3aa8caafb75452f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26d03f7f655442afb23738bb3107f0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962a3eb864304d35ad50d6985e973334",
            "placeholder": "​",
            "style": "IPY_MODEL_2330a69f1da749b3aa8caafb75452f4b",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "55f81c6e7b8f40349c1c861da7537acf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798f2df6745945eb8e1f77860104af30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e2c1ca40e894658aa7d91cb7b11dc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "962a3eb864304d35ad50d6985e973334": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5604096cec347d3ae35cffe92c7e36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def9fcd1f79f44db86bfe38778597dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_8e2c1ca40e894658aa7d91cb7b11dc83",
            "value": " 714M/714M [00:38&lt;00:00, 20.3MB/s]"
          }
        },
        "def9fcd1f79f44db86bfe38778597dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c3398925a2458997742c6c47538c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26d03f7f655442afb23738bb3107f0b2",
              "IPY_MODEL_0cecf36c5906483a9dd288ffe365b4cf",
              "IPY_MODEL_d5604096cec347d3ae35cffe92c7e36c"
            ],
            "layout": "IPY_MODEL_55f81c6e7b8f40349c1c861da7537acf"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}