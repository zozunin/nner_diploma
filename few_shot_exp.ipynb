{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Все чекпойнты для этой задачи можно скачать по ссылке: https://drive.google.com/drive/folders/13wo8CohBcwmFZHUA06Dl_PISnYfahuXm?usp=sharing\n",
    "    * Предполагается, что лежат в папке all_data/fs_checkpoints (далее описано, для какого эксперимента используется конкретный чекпойнт)\n",
    "    * Дополнительно в формате npy с тем же префиксом, что и чекпойнт, лежат сохраненные вектора для некоторых просчетов, можно загрузить их\n",
    "    *  \n",
    "    \n",
    "\n",
    "* Данные для обучения по ссылке (кладутся в основную папку all_data): https://drive.google.com/drive/folders/1oHRzegJm9tId0jliPQPq5G7bYwtwVEK7?usp=sharing\n",
    "\n",
    "* Для одного из экспериментов требуется предобученная на общей задаче биаффинная модель: сохранить biaffine_checkpoint.pth по пути all_data/general_checkpoints/: https://drive.google.com/drive/folders/1ywaDZ4xX_ahcf0jWMJX69TDMdMhMqtmY?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_utils.dataset_process import get_ind_sequence, dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from scripts.span_classifier import NNERModel\n",
    "from scripts.task_trainers import FSTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda, optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED=42):\n",
    "    \n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Загрузка данных для обучения и валидации"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перфикс уточняет тип используемых данных\n",
    "* fs - для начала обучения только по целевым классам или для этапа тестирования\n",
    "* без префикса - для начала обучения по всем размеченным данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_labels2ids = {'Trash': 0,\n",
    "                'DISEASE': 1,\n",
    "                'PENALTY': 2,\n",
    "                'WORK_OF_ART': 3}\n",
    "fs_ids2labels = {v:k for k, v in fs_labels2ids.items()}\n",
    "\n",
    "from scripts.data_utils.label_helper import labels2ids, ids2labels "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренировочные тексты\n",
    "ner_ds_path = 'all_data/train_texts.csv'\n",
    "train_texts = pd.read_csv(ner_ds_path, sep=';')\n",
    "\n",
    "to_del = [idx for idx, sent in enumerate(train_texts.Contents) if re.match(r'^\\s+$', sent)]\n",
    "train_texts = train_texts.drop(to_del).reset_index()\n",
    "train_texts = train_texts.drop('index', axis=1)\n",
    "train_inds = get_ind_sequence(train_texts)\n",
    "\n",
    "# тренировочные лейблы\n",
    "ner_ds_path = 'all_data/train_spans.csv'\n",
    "train_spans = pd.read_csv(ner_ds_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_ds = dataset(train_texts, max_len=284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_inds(data_texts, data_spans):\n",
    "    # отбор предложений с нужными few-shot классами для обучения\n",
    "    address = []\n",
    "    for row in range(len(data_spans)):\n",
    "        l = data_spans.Spans.iloc[row].find(\"'\")\n",
    "        r = data_spans.Spans.iloc[row].rfind(\"'\")\n",
    "        temp_class = data_spans.Spans.iloc[row][l+1:r]\n",
    "        if temp_class in fs_labels2ids.keys():\n",
    "            temp_ad = [data_spans.TextID.iloc[row], data_spans.SentID.iloc[row]]\n",
    "            if temp_ad not in address:\n",
    "                address.append(temp_ad)\n",
    "\n",
    "    tiny_dataset_ids = []\n",
    "    for row in range(len(data_texts)):\n",
    "        if [data_texts.TextID.iloc[row], data_texts.SentID.iloc[row]] in address:\n",
    "            tiny_dataset_ids.append(row)\n",
    "    return tiny_dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для few-shot только предложения с нужными классами\n",
    "tiny_dataset_ids = get_few_inds(train_texts, train_spans)\n",
    "fs_train_texts = train_texts.iloc[tiny_dataset_ids].reset_index()\n",
    "fs_train_inds = train_inds.iloc[tiny_dataset_ids].reset_index()\n",
    "\n",
    "fs_train_texts_ds = dataset(fs_train_texts, max_len=284)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_ds_path = 'all_data/test_texts.csv'\n",
    "\n",
    "test_texts = pd.read_csv(ner_ds_path, sep=';')\n",
    "to_del = [idx for idx, sent in enumerate(test_texts.Contents) if re.match(r'^\\s+$', sent)]\n",
    "test_texts = test_texts.drop(to_del).reset_index()\n",
    "test_texts = test_texts.drop('index', axis=1)\n",
    "test_inds = get_ind_sequence(test_texts)\n",
    "\n",
    "ner_ds_path = 'all_data/test_spans.csv'\n",
    "test_spans = pd.read_csv(ner_ds_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_ds = dataset(test_texts, max_len=284)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обучение\n",
    "\n",
    "Валидация моделей запускается в следующей секции"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется модель, давшая лучшие результаты на общей задаче - биаффинная, все настройки для нее"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество эпох зависит от метода обучения\n",
    "* Если требуется проход по всем размеченным данным (все классы общей задачи), что хватает 20 эпох (use_all_data = True)\n",
    "* Если проход только по few-shot данным - 200 эпох (use_all_data = False)\n",
    "\n",
    "Обучение для задачи проводилось в гугл колабе, поэтому для каждого нового этапа подгружалась лучшая модель с предыдущего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь для обучения сначала на полном проходе по размеченным данным\n",
    "use_all_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 1,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "\n",
    "training_loader = DataLoader(train_texts_ds, **train_params)\n",
    "testing_loader = DataLoader(test_texts_ds, **test_params)\n",
    "\n",
    "fs_training_loader = DataLoader(fs_train_texts_ds, **train_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При запуске обучения с полных данных, после которых требуется дообучения на few_shot задаче повторный запуск с ячейки ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_all_data==True:\n",
    "    use_ids2labels = ids2labels\n",
    "    use_labels2ids = labels2ids\n",
    "elif use_all_data==False:\n",
    "    use_ids2labels = fs_ids2labels\n",
    "    use_labels2ids = fs_labels2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = NNERModel(num_labels=len(use_ids2labels),\n",
    "                  device=device,\n",
    "                  max_seq_len=284,\n",
    "                  max_span_len=20,\n",
    "                  num_heads=None,\n",
    "                  extractor_type='biaffine',\n",
    "                  mode='extraction',\n",
    "                  extractor_use_gcn=False)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-05\n",
    "if use_all_data == True:\n",
    "    EPOCHS = 20\n",
    "else:\n",
    "    EPOCHS = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),\n",
    "                        lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(training_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если с круговой ошибкой обучение должно происходить при полном проходе по данным  со всеми метками классов (**use_all_data==True**), то обучение всегда начинается со свежего сохранения (**from_start = True**), но может быть с предварительным подтягиванием весов модели, обученных для общей задачи (**from_biaf_mode = True**)\n",
    "\n",
    "Если должны использоваться исключительно few-shot классы, то обучение может быть с самого начала (**from_start = True**), а также с биаффинной моделью (**from_biaf_mode = True**) или c сохранения с полного прохода - тогда **from_start=False**\n",
    "\n",
    "Для продолжения обучения после полного прохода на few-shot данных требуется указать путь до лучшей сохраненной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_start = True\n",
    "from_biaf_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_prefix = 'testing_fs_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проход сначала по полным данным\n",
    "if use_all_data == True:\n",
    "    from_start = True\n",
    "\n",
    "# проход по few-shot примерам\n",
    "if use_all_data == False:\n",
    "    if from_start == False:\n",
    "        from_biaf_mode = False\n",
    "        check_path = 'new_data/{safe_prefix}_checkpoint.pth'\n",
    "        checkpoint = torch.load(check_path)\n",
    "        model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if from_biaf_mode == True:\n",
    "    bf_checkpoint = torch.load(f'all_data/general_checkpoints/biaffine_checkpoint.pth')\n",
    "    # отбрасываем классифицирующий слой\n",
    "    drop_layers = list(bf_checkpoint.keys())[-4:]\n",
    "    for i in drop_layers:\n",
    "        if i in bf_checkpoint:\n",
    "            del bf_checkpoint[i]\n",
    "\n",
    "    # подгрузка весов в новую модель\n",
    "    n_model = model.state_dict()\n",
    "    for layer, params in bf_checkpoint.items():\n",
    "        if layer in n_model.keys():\n",
    "            n_model[layer] = params\n",
    "    model.load_state_dict(n_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nner_trainer = FSTrainer(model=model,\n",
    "                       device = device,\n",
    "                       ids2labels=use_ids2labels,\n",
    "                       labels2ids=use_labels2ids,\n",
    "                       optimizer=optimizer,\n",
    "                       scheduler=scheduler,\n",
    "                       )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначить название файла для сохранения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_prefix = 'testing_fs_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch+1}/{EPOCHS}')\n",
    "    if use_all_data == True:\n",
    "        epoch_loss = nner_trainer(training_loader, train_spans, train_inds, mode='train')\n",
    "    elif use_all_data == False:\n",
    "        epoch_loss = nner_trainer(fs_training_loader, train_spans, fs_train_inds, mode='train')\n",
    "\n",
    "    if epoch == 0:\n",
    "        last_best = epoch_loss\n",
    "\n",
    "    if epoch_loss < last_best:\n",
    "        last_best = epoch_loss\n",
    "        check_path = f'new_data/{safe_prefix}_checkpoint.pth'\n",
    "        torch.save(model.state_dict(),  check_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения требуется подгрузить новую модель либо для тестирования (далее), либо для нового прохода по малым данным - для этого нужно вернуться назад и запустить заново код с нужными флагами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление модели для повторной инициализации\n",
    "del model\n",
    "# False для продолжения на задаче с малыми классами\n",
    "use_all_data = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Валидация на тесте с подгружаемой моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ids2labels = fs_ids2labels\n",
    "use_labels2ids = fs_labels2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = NNERModel(num_labels=len(use_ids2labels),\n",
    "                  device=device,\n",
    "                  max_seq_len=284,\n",
    "                  max_span_len=20,\n",
    "                  num_heads=None,\n",
    "                  extractor_type='biaffine',\n",
    "                  mode='extraction',\n",
    "                  extractor_use_gcn=False)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобученные модели для каждого подхода в папке all_data/fs_checkpoints/\n",
    "\n",
    "Варианты обучения и соответствующее сохранение:\n",
    "\n",
    "1. Инициализация модели с нуля, проход только по few-shot классам (use_all_data=False) - fs1\n",
    "2. Инициализация модели с предобученными на общей задаче весами (from_biaf_mode=True) с проходом только по few-shot классам (use_all_data=False) - fs2\n",
    "3. Инициализация модели с нуля, проход сначала по всем данным со всеми размеченными классами, потом - только по few-shot для донастройки - fs3\n",
    "4. Инициализация модели с предобученными на общей задаче весами, проход по всем размеченным, потом - только по few-shot - fs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_prefix = 'fs3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f'all_data/fs_checkpoints/{safe_prefix}_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nner_trainer = FSTrainer(model=model,\n",
    "                       device = device,\n",
    "                       ids2labels=use_ids2labels,\n",
    "                       labels2ids=use_labels2ids\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 1,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "fs_training_loader = DataLoader(fs_train_texts_ds, **train_params)\n",
    "testing_loader = DataLoader(test_texts_ds, **test_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вектора для сущностей с помощью предобученной модели\n",
    "\n",
    "При повторном запуске из-за рандомного выбора отрицательного класса результаты могут отличаться, для сравнения можно использовать сохранения отрезков (сохранения в формате npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить готовое\n",
    "# path_to_train_spans = f'all_data/fs_checkpoints/{safe_prefix}_spans.npy'\n",
    "# spans = np.load(path_to_train_spans, allow_pickle=True)\n",
    "\n",
    "# path_to_train_labels = f'all_data/fs_checkpoints/{safe_prefix}_labels.npy'\n",
    "# labels = np.load(path_to_train_labels, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:03, 15.26it/s]\n"
     ]
    }
   ],
   "source": [
    "spans, labels = nner_trainer(fs_training_loader, train_spans, fs_train_inds, mode='get_spans')\n",
    "\n",
    "# сохранение\n",
    "# spans = np.array(spans)\n",
    "# with open(f'all_data/fs_checkpoints/{safe_prefix}_spans.npy', 'wb') as f:\n",
    "#         np.save(f, spans)\n",
    "# labels = np.array(labels)\n",
    "# with open(f'all_data/fs_checkpoints/{safe_prefix}_labels.npy', 'wb') as f:\n",
    "#         np.save(f, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация KNN, k - число соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nner_trainer.init_knn(spans, labels, k=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результирующая таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "935it [05:58,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************SUB NER TYPES********************\n",
      "DISEASE F1: 0.02467%\n",
      "DISEASE Recall: 50.87719%\n",
      "DISEASE Precision: 0.01234%\n",
      "PENALTY F1: 0.54146%\n",
      "PENALTY Recall: 44.44444%\n",
      "PENALTY Precision: 0.27239%\n",
      "WORK_OF_ART F1: 32.09877%\n",
      "WORK_OF_ART Recall: 55.91398%\n",
      "WORK_OF_ART Precision: 22.51082%\n",
      "Macro F1: 10.88830%\n"
     ]
    }
   ],
   "source": [
    "_ = nner_trainer(testing_loader, test_spans, test_inds, mode='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
